{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"The WLCG Virtual Organization An instance of IAM has been deployed in support of WLCG development and integration activities in support of the migration to token-based authentication and authorization. The WLCG IAM instance is integrated with CERN SSO. Registration is active. IAM documentation is available here . Clients applications can be registered following these instructions , or using oidc-agent . VOMS support is enabled. To link an X.509 certificate to an existing IAM WLCG account, follow these instructions . As in VOMS, multiple certificates can be linked to an account. WLCG VO VOMS configuration New LSC files will be rolled out on Nov. 22nd, 2021 On Monday Nov. 22nd, 2021 at 11 we will update the WLCC VO LSC configuration. The reason for the update is that the current certificate is going to expire and that the Sectigo CA INFN currently uses has changed the structure of the DNs of the issued certificates. The updated configuration that you find here is compliant with the new certificate. Do not roll out this configuration before Nov. 22nd, or VOMS validation will break at your site . VOMSES LSC To have a working VOMS configuration for the WLCG VO: place the lsc file in the /etc/grid-security/vomsdir/wlcg directory place the vomses file in the /etc/vomses directory (only needed if you need to do voms-proxy-init ) RPM installation You can use the following RPM package to enable support for the WLCG VO on your RHEL machine: RPM package VOMS clients compatibility The latest supported VOMS clients are required. Also note that this VO is supported by IAM, i.e. there are no VOMS Admin endpoints that can be used to generate Gridmap files. Example $ voms-proxy-init -voms wlcg Enter GRID pass phrase for this identity: Contacting wlcg-voms.cloud.cnaf.infn.it:15001 [/DC=org/DC=terena/DC=tcs/C=IT/L=Frascati/O=Istituto Nazionale di Fisica Nucleare/CN=voms-wlcg.cloud.cnaf.infn.it] \"wlcg\"... Remote VOMS server contacted succesfully. Created proxy in /tmp/x509up_u501. Your proxy is valid until Thu Jan 30 03:47:58 CET 2020 $ voms-proxy-info -all subject : /DC=org/DC=terena/DC=tcs/C=IT/O=Istituto Nazionale di Fisica Nucleare/CN=Andrea Ceccanti aceccant@infn.it/CN=743668640 issuer : /DC=org/DC=terena/DC=tcs/C=IT/O=Istituto Nazionale di Fisica Nucleare/CN=Andrea Ceccanti aceccant@infn.it identity : /DC=org/DC=terena/DC=tcs/C=IT/O=Istituto Nazionale di Fisica Nucleare/CN=Andrea Ceccanti aceccant@infn.it type : RFC3820 compliant impersonation proxy strength : 1024 path : /tmp/x509up_u501 timeleft : 11:59:55 key usage : Digital Signature, Key Encipherment, Data Encipherment === VO wlcg extension information === VO : wlcg subject : /DC=org/DC=terena/DC=tcs/C=IT/O=Istituto Nazionale di Fisica Nucleare/CN=Andrea Ceccanti aceccant@infn.it issuer : /DC=org/DC=terena/DC=tcs/C=IT/L=Frascati/O=Istituto Nazionale di Fisica Nucleare/CN=wlcg-voms.cloud.cnaf.infn.it attribute : /wlcg attribute : /wlcg/xfers timeleft : 11:59:55 uri : wlcg-voms.cloud.cnaf.infn.it:15001","title":"The WLCG Virtual Organization"},{"location":"#the-wlcg-virtual-organization","text":"An instance of IAM has been deployed in support of WLCG development and integration activities in support of the migration to token-based authentication and authorization. The WLCG IAM instance is integrated with CERN SSO. Registration is active. IAM documentation is available here . Clients applications can be registered following these instructions , or using oidc-agent . VOMS support is enabled. To link an X.509 certificate to an existing IAM WLCG account, follow these instructions . As in VOMS, multiple certificates can be linked to an account.","title":"The WLCG Virtual Organization"},{"location":"#wlcg-vo-voms-configuration","text":"New LSC files will be rolled out on Nov. 22nd, 2021 On Monday Nov. 22nd, 2021 at 11 we will update the WLCC VO LSC configuration. The reason for the update is that the current certificate is going to expire and that the Sectigo CA INFN currently uses has changed the structure of the DNs of the issued certificates. The updated configuration that you find here is compliant with the new certificate. Do not roll out this configuration before Nov. 22nd, or VOMS validation will break at your site . VOMSES LSC To have a working VOMS configuration for the WLCG VO: place the lsc file in the /etc/grid-security/vomsdir/wlcg directory place the vomses file in the /etc/vomses directory (only needed if you need to do voms-proxy-init )","title":"WLCG VO VOMS configuration"},{"location":"#rpm-installation","text":"You can use the following RPM package to enable support for the WLCG VO on your RHEL machine: RPM package","title":"RPM installation"},{"location":"#voms-clients-compatibility","text":"The latest supported VOMS clients are required. Also note that this VO is supported by IAM, i.e. there are no VOMS Admin endpoints that can be used to generate Gridmap files.","title":"VOMS clients compatibility"},{"location":"#example","text":"$ voms-proxy-init -voms wlcg Enter GRID pass phrase for this identity: Contacting wlcg-voms.cloud.cnaf.infn.it:15001 [/DC=org/DC=terena/DC=tcs/C=IT/L=Frascati/O=Istituto Nazionale di Fisica Nucleare/CN=voms-wlcg.cloud.cnaf.infn.it] \"wlcg\"... Remote VOMS server contacted succesfully. Created proxy in /tmp/x509up_u501. Your proxy is valid until Thu Jan 30 03:47:58 CET 2020 $ voms-proxy-info -all subject : /DC=org/DC=terena/DC=tcs/C=IT/O=Istituto Nazionale di Fisica Nucleare/CN=Andrea Ceccanti aceccant@infn.it/CN=743668640 issuer : /DC=org/DC=terena/DC=tcs/C=IT/O=Istituto Nazionale di Fisica Nucleare/CN=Andrea Ceccanti aceccant@infn.it identity : /DC=org/DC=terena/DC=tcs/C=IT/O=Istituto Nazionale di Fisica Nucleare/CN=Andrea Ceccanti aceccant@infn.it type : RFC3820 compliant impersonation proxy strength : 1024 path : /tmp/x509up_u501 timeleft : 11:59:55 key usage : Digital Signature, Key Encipherment, Data Encipherment === VO wlcg extension information === VO : wlcg subject : /DC=org/DC=terena/DC=tcs/C=IT/O=Istituto Nazionale di Fisica Nucleare/CN=Andrea Ceccanti aceccant@infn.it issuer : /DC=org/DC=terena/DC=tcs/C=IT/L=Frascati/O=Istituto Nazionale di Fisica Nucleare/CN=wlcg-voms.cloud.cnaf.infn.it attribute : /wlcg attribute : /wlcg/xfers timeleft : 11:59:55 uri : wlcg-voms.cloud.cnaf.infn.it:15001","title":"Example"},{"location":"token-based-authorization/","text":"Token-based authorization Getting an account on the wlcg IAM instance To register in the WLCG IAM instance, point your browser to the wlcg IAM VO page . If you have a CERN account, use that account to login to the wlcg VO. Registering a client in the WLCG IAM instance The recommended way of registering a client in the WLCG IAM instance is to use oidc-agent as described here . By default, clients registered in the WLCG IAM instance will use the common WLCG JWT profile . WLCG IAM scope policy configuration IAM allows to define policies to limit access to scopes only to selected classes of users. In the IAM wlcg instance policies are defined to limit access to compute.* and storage.* scopes, which are the scopes defined in the WLCG JWT profile to control access to compute and storage resources. In more detail: access to the compute.create , compute.read , compute.cancel , compute.modify scope is only granted to members of the wlcg/pilots group; access to the storage.read , storage.modify , storage.create scopes is only granted to members of the wlcg/xfers group. Membership in those groups can be requested, after registration, from the IAM dashboard. Getting tokens out IAM Scope-based authorization In order to transfer data across WLCG storage elements using scope-based authorization, request a token with the 'storage.read:/' and 'storage.modify:/' scopes, as follows: $ AT=$(oidc-token -s storage.read:/ -s storage.modify:/ wlcg) You can use the jwt utility to inspect the token contents: $ echo $AT | jwt ... \u273b Payload { \"wlcg.ver\": \"1.0\", \"sub\": \"a1b98335-9649-4fb0-961d-5a49ce108d49\", \"aud\": \"https://wlcg.cern.ch/jwt/v1/any\", \"nbf\": 1593004542, \"scope\": \"storage.read:/ storage.modify:/\", \"iss\": \"https://wlcg.cloud.cnaf.infn.it/\", \"exp\": 1593008142, \"iat\": 1593004542, \"jti\": \"da0a2f89-3cbf-42a7-9403-0b43d814551d\", \"client_id\": \"edfacfb1-f59d-44d0-9eb6-a745ac52f462\" } or directly decode token payload with echo $AT | sed 's/.*\\.\\(.*\\)\\..*/\\1==/' | base64 -d | jq Then you can use davix or curl to access WLCG storage: $ davix-ls -l --capath /etc/grid-security/certificates -H \"Authorization: Bearer ${AT}\" https://xfer.cr.cnaf.infn.it:8443/wlcg/ drwxrwxrwx 0 0 2020-06-24 10:41:13 wlcgdoma -rwxrwxrwx 0 1048576 2020-06-18 11:16:25 1M drwxrwxrwx 0 0 2020-04-08 14:56:56 https drwxrwxrwx 0 0 2020-06-24 15:18:36 test-andrea $ davix-put --capath /etc/grid-security/certificates -H \"Authorization: Bearer ${AT}\" /etc/services https://prometheus.desy.de:2443/VOs/wlcg/file1 $ davix-get --capath /etc/grid-security/certificates https://prometheus.desy.de:2443/VOs/wlcg/file1 /tmp/x Performing Read operation on: http://[2001:638:700:1005:0:0:1:95]:21881/VOs/wlcg/file1?dcache-http-uuid=679aa9a4-b946-49fd-8522-156d165425c5 [==================================] 100% 654KiB/654KiB 0B/s $ davix-mkdir --capath /etc/grid-security/certificates -H \"Authorization: Bearer ${AT}\" https://golias100.farm.particle.cz/dpm/farm.particle.cz/home/wlcg/dir1 $ davix-cp --capath /etc/grid-security/certificates -H \"Authorization: Bearer ${AT}\" -H \"TransferHeaderAuthorization: Bearer ${AT}\" --copy-mode pull https://prometheus.desy.de:2443/VOs/wlcg/file1 https://golias100.farm.particle.cz/dpm/farm.particle.cz/home/wlcg/dir1/file1.pull $ davix-cp --capath /etc/grid-security/certificates -H \"Authorization: Bearer ${AT}\" -H \"TransferHeaderAuthorization: Bearer ${AT}\" --copy-mode pull https://prometheus.desy.de:2443/VOs/wlcg/file1 https://golias100.farm.particle.cz/dpm/farm.particle.cz/home/wlcg/dir1/file1.push $ davix-rm --capath /etc/grid-security/certificates -H \"Authorization: Bearer ${AT}\" https://golias100.farm.particle.cz/dpm/farm.particle.cz/home/wlcg/dir1 # Create directory $ curl --capath /etc/grid-security/certificates -X MKCOL -H \"Authorization: Bearer ${AT}\" https://golias100.farm.particle.cz/dpm/farm.particle.cz/home/wlcg/dir1 # Upload file $ curl --capath /etc/grid-security/certificates -L -X PUT -H \"Authorization: Bearer ${AT}\" --upload-file /etc/services https://golias100.farm.particle.cz/dpm/farm.particle.cz/home/wlcg/dir1/file1 # Download file $ curl --capath /etc/grid-security/certificates -L -X GET -H \"Authorization: Bearer ${AT}\" --output /tmp/x https://golias100.farm.particle.cz/dpm/farm.particle.cz/home/wlcg/dir1/file1 # TPC pull $ curl --capath /etc/grid-security/certificates -L -X COPY -H 'Credentials: none' -H \"Authorization: Bearer ${AT}\" -H \"TransferHeaderAuthorization: Bearer ${AT}\" -H \"Source: https://golias100.farm.particle.cz/dpm/farm.particle.cz/home/wlcg/dir1/file1\" https://golias100.farm.particle.cz/dpm/farm.particle.cz/home/wlcg/dir1/file2 # TPC push $ curl --capath /etc/grid-security/certificates -L -X COPY -H 'Credentials: none' -H \"Authorization: Bearer ${AT}\" -H \"TransferHeaderAuthorization: Bearer ${AT}\" -H \"Destination: https://golias100.farm.particle.cz/dpm/farm.particle.cz/home/wlcg/dir1/file3\" https://golias100.farm.particle.cz/dpm/farm.particle.cz/home/wlcg/dir1/file1 # Delete $ curl --capath /etc/grid-security/certificates -L -X DELETE -H \"Authorization: Bearer ${AT}\" https://golias100.farm.particle.cz/dpm/farm.particle.cz/home/wlcg/dir1 Gfal python API can be also used with tokens, e.g. import os import gfal2 ctx = gfal2.creat_context() cred = gfal2.cred_new(\"BEARER\", os.getenv('TOKEN')) gfal2.cred_set(ctx, 'amnesiac.cloud.cnaf.infn.it', cred) ctx.stat('https://amnesiac.cloud.cnaf.infn.it:8443/wlcg/wlcgdoma/https/jwttest/1M') Group-based authorization TBD","title":"Token-based authorization"},{"location":"token-based-authorization/#token-based-authorization","text":"","title":"Token-based authorization"},{"location":"token-based-authorization/#getting-an-account-on-the-wlcg-iam-instance","text":"To register in the WLCG IAM instance, point your browser to the wlcg IAM VO page . If you have a CERN account, use that account to login to the wlcg VO.","title":"Getting an account on the wlcg IAM instance"},{"location":"token-based-authorization/#registering-a-client-in-the-wlcg-iam-instance","text":"The recommended way of registering a client in the WLCG IAM instance is to use oidc-agent as described here . By default, clients registered in the WLCG IAM instance will use the common WLCG JWT profile .","title":"Registering a client in the WLCG IAM instance"},{"location":"token-based-authorization/#wlcg-iam-scope-policy-configuration","text":"IAM allows to define policies to limit access to scopes only to selected classes of users. In the IAM wlcg instance policies are defined to limit access to compute.* and storage.* scopes, which are the scopes defined in the WLCG JWT profile to control access to compute and storage resources. In more detail: access to the compute.create , compute.read , compute.cancel , compute.modify scope is only granted to members of the wlcg/pilots group; access to the storage.read , storage.modify , storage.create scopes is only granted to members of the wlcg/xfers group. Membership in those groups can be requested, after registration, from the IAM dashboard.","title":"WLCG IAM scope policy configuration"},{"location":"token-based-authorization/#getting-tokens-out-iam","text":"","title":"Getting tokens out IAM"},{"location":"token-based-authorization/#scope-based-authorization","text":"In order to transfer data across WLCG storage elements using scope-based authorization, request a token with the 'storage.read:/' and 'storage.modify:/' scopes, as follows: $ AT=$(oidc-token -s storage.read:/ -s storage.modify:/ wlcg) You can use the jwt utility to inspect the token contents: $ echo $AT | jwt ... \u273b Payload { \"wlcg.ver\": \"1.0\", \"sub\": \"a1b98335-9649-4fb0-961d-5a49ce108d49\", \"aud\": \"https://wlcg.cern.ch/jwt/v1/any\", \"nbf\": 1593004542, \"scope\": \"storage.read:/ storage.modify:/\", \"iss\": \"https://wlcg.cloud.cnaf.infn.it/\", \"exp\": 1593008142, \"iat\": 1593004542, \"jti\": \"da0a2f89-3cbf-42a7-9403-0b43d814551d\", \"client_id\": \"edfacfb1-f59d-44d0-9eb6-a745ac52f462\" } or directly decode token payload with echo $AT | sed 's/.*\\.\\(.*\\)\\..*/\\1==/' | base64 -d | jq Then you can use davix or curl to access WLCG storage: $ davix-ls -l --capath /etc/grid-security/certificates -H \"Authorization: Bearer ${AT}\" https://xfer.cr.cnaf.infn.it:8443/wlcg/ drwxrwxrwx 0 0 2020-06-24 10:41:13 wlcgdoma -rwxrwxrwx 0 1048576 2020-06-18 11:16:25 1M drwxrwxrwx 0 0 2020-04-08 14:56:56 https drwxrwxrwx 0 0 2020-06-24 15:18:36 test-andrea $ davix-put --capath /etc/grid-security/certificates -H \"Authorization: Bearer ${AT}\" /etc/services https://prometheus.desy.de:2443/VOs/wlcg/file1 $ davix-get --capath /etc/grid-security/certificates https://prometheus.desy.de:2443/VOs/wlcg/file1 /tmp/x Performing Read operation on: http://[2001:638:700:1005:0:0:1:95]:21881/VOs/wlcg/file1?dcache-http-uuid=679aa9a4-b946-49fd-8522-156d165425c5 [==================================] 100% 654KiB/654KiB 0B/s $ davix-mkdir --capath /etc/grid-security/certificates -H \"Authorization: Bearer ${AT}\" https://golias100.farm.particle.cz/dpm/farm.particle.cz/home/wlcg/dir1 $ davix-cp --capath /etc/grid-security/certificates -H \"Authorization: Bearer ${AT}\" -H \"TransferHeaderAuthorization: Bearer ${AT}\" --copy-mode pull https://prometheus.desy.de:2443/VOs/wlcg/file1 https://golias100.farm.particle.cz/dpm/farm.particle.cz/home/wlcg/dir1/file1.pull $ davix-cp --capath /etc/grid-security/certificates -H \"Authorization: Bearer ${AT}\" -H \"TransferHeaderAuthorization: Bearer ${AT}\" --copy-mode pull https://prometheus.desy.de:2443/VOs/wlcg/file1 https://golias100.farm.particle.cz/dpm/farm.particle.cz/home/wlcg/dir1/file1.push $ davix-rm --capath /etc/grid-security/certificates -H \"Authorization: Bearer ${AT}\" https://golias100.farm.particle.cz/dpm/farm.particle.cz/home/wlcg/dir1 # Create directory $ curl --capath /etc/grid-security/certificates -X MKCOL -H \"Authorization: Bearer ${AT}\" https://golias100.farm.particle.cz/dpm/farm.particle.cz/home/wlcg/dir1 # Upload file $ curl --capath /etc/grid-security/certificates -L -X PUT -H \"Authorization: Bearer ${AT}\" --upload-file /etc/services https://golias100.farm.particle.cz/dpm/farm.particle.cz/home/wlcg/dir1/file1 # Download file $ curl --capath /etc/grid-security/certificates -L -X GET -H \"Authorization: Bearer ${AT}\" --output /tmp/x https://golias100.farm.particle.cz/dpm/farm.particle.cz/home/wlcg/dir1/file1 # TPC pull $ curl --capath /etc/grid-security/certificates -L -X COPY -H 'Credentials: none' -H \"Authorization: Bearer ${AT}\" -H \"TransferHeaderAuthorization: Bearer ${AT}\" -H \"Source: https://golias100.farm.particle.cz/dpm/farm.particle.cz/home/wlcg/dir1/file1\" https://golias100.farm.particle.cz/dpm/farm.particle.cz/home/wlcg/dir1/file2 # TPC push $ curl --capath /etc/grid-security/certificates -L -X COPY -H 'Credentials: none' -H \"Authorization: Bearer ${AT}\" -H \"TransferHeaderAuthorization: Bearer ${AT}\" -H \"Destination: https://golias100.farm.particle.cz/dpm/farm.particle.cz/home/wlcg/dir1/file3\" https://golias100.farm.particle.cz/dpm/farm.particle.cz/home/wlcg/dir1/file1 # Delete $ curl --capath /etc/grid-security/certificates -L -X DELETE -H \"Authorization: Bearer ${AT}\" https://golias100.farm.particle.cz/dpm/farm.particle.cz/home/wlcg/dir1 Gfal python API can be also used with tokens, e.g. import os import gfal2 ctx = gfal2.creat_context() cred = gfal2.cred_new(\"BEARER\", os.getenv('TOKEN')) gfal2.cred_set(ctx, 'amnesiac.cloud.cnaf.infn.it', cred) ctx.stat('https://amnesiac.cloud.cnaf.infn.it:8443/wlcg/wlcgdoma/https/jwttest/1M')","title":"Scope-based authorization"},{"location":"token-based-authorization/#group-based-authorization","text":"TBD","title":"Group-based authorization"},{"location":"token-based-authorization/compliance/","text":"WLCG JWT compliance Joining compliance tests By sending mail to wlcg-doma-tpc e-group with details how to access your storage endpoint. Storage pre-requirements Storage endpoint used in these tests must follow testsuite storage area configuration pre-requisites . Results WLCG JWT compliance test suite runs every day at 12 UTC and results can be accessed using CNAF Jenkins dashboard.","title":"WLCG JWT compliance"},{"location":"token-based-authorization/compliance/#wlcg-jwt-compliance","text":"","title":"WLCG JWT compliance"},{"location":"token-based-authorization/compliance/#joining-compliance-tests","text":"By sending mail to wlcg-doma-tpc e-group with details how to access your storage endpoint.","title":"Joining compliance tests"},{"location":"token-based-authorization/compliance/#storage-pre-requirements","text":"Storage endpoint used in these tests must follow testsuite storage area configuration pre-requisites .","title":"Storage pre-requirements"},{"location":"token-based-authorization/compliance/#results","text":"WLCG JWT compliance test suite runs every day at 12 UTC and results can be accessed using CNAF Jenkins dashboard.","title":"Results"},{"location":"token-based-authorization/doma-testbed/","text":"DOMA Token-based AuthZ Testbed Joining testbed Send mail to wlcg-doma-tpc e-group if you want to join Rucio DOMA Functional Tests OIDC or advertise your CE that support job submission with tokens. Please include necessary to access your SE/CE endpoint, as an example you can use entries that were already filled in tables with available resources that to some degree already supports tokens. IAM To be able to use tokens it is necessary to know which token issuer is used by individual VOs. For tests we use WLCG IAM token issuer which provides services for \"wlcg\" VO, e.g. VO Issuer WLCG IAM https://wlcg.cloud.cnaf.infn.it/ ATLAS IAM https://atlas-auth.web.cern.ch/ CMS IAM https://cms-auth.web.cern.ch/ DUNE https://cilogon.org/dune Fermilab https://cilogon.org/fermilab OSG have token isser details stored with all VO detail in their virtual-organization topology files. WLCG/EGI currently doesn't provide common (trusted) place with token isser details, but naturally this could become part of EGI VO ID Cards (briefly touched this topic here ). CE ARC-CE ARC-CE 6.12 still has limited support for WLCG JWT tokens and they can be only used to submit jobs with HTTP based protocols ( EMI-ES and REST interface). With current limitations configuration close to expectations from WLCG JWT profile could look like (replace arc1.example.com with your ARC-CE hostname or arc1.example.com:port_number if you don't use standard HTTPS port 443 ): # ... [authtokens] [authgroup: wlcg_iam] # capability based authorization that use compute.* scopes authtokens = * https://wlcg.cloud.cnaf.infn.it/ https://arc1.example.com compute.create * authtokens = * https://wlcg.cloud.cnaf.infn.it/ https://arc1.example.com compute.read * authtokens = * https://wlcg.cloud.cnaf.infn.it/ https://arc1.example.com compute.modify * authtokens = * https://wlcg.cloud.cnaf.infn.it/ https://arc1.example.com compute.cancel * # group based authorization that use /wlcg/pilots group (LHC experiments prefer capabilities) authtokens = * https://wlcg.cloud.cnaf.infn.it/ https://arc1.example.com * /wlcg/pilots # accept token issued by EGI Check-in for job submission (both old MitreID and new Keycloak issuer) [authgroup: egi_aai] # very rough / DANGEROUS configuration - accepting all tokens without restrictions #authtokens = * https://aai.egi.eu/oidc/ * * * #authtokens = * https://aai.egi.eu/auth/realms/egi * * * # it is possible to restrict job submission to the specific EGI user authtokens = 85ff127e07ea6660c727831b99e18e4e96b319283f8d2cc8113f405bad2ba261@egi.eu https://aai.egi.eu/oidc/ * * * authtokens = 85ff127e07ea6660c727831b99e18e4e96b319283f8d2cc8113f405bad2ba261@egi.eu https://aai.egi.eu/auth/realms/egi * * * # just an example for ARC-CE running on arc1.example.com # recommendation for ATLAS configuration may change in fugure # (this is not the official ATLAS site configuration documentation) [authgroup: atlas_iam_prd] authtokens = 7dee38a3-6ab8-4fe2-9e4c-58039c21d817 https://atlas-auth.web.cern.ch/ https://arc1.example.com compute.create * authtokens = 7dee38a3-6ab8-4fe2-9e4c-58039c21d817 https://atlas-auth.web.cern.ch/ https://arc1.example.com compute.read * authtokens = 7dee38a3-6ab8-4fe2-9e4c-58039c21d817 https://atlas-auth.web.cern.ch/ https://arc1.example.com compute.modify * authtokens = 7dee38a3-6ab8-4fe2-9e4c-58039c21d817 https://atlas-auth.web.cern.ch/ https://arc1.example.com compute.cancel * [authgroup: atlas_iam_plt] authtokens = 750e9609-485a-4ed4-bf16-d5cc46c71024 https://atlas-auth.web.cern.ch/ https://arc1.example.com compute.create * authtokens = 750e9609-485a-4ed4-bf16-d5cc46c71024 https://atlas-auth.web.cern.ch/ https://arc1.example.com compute.read * authtokens = 750e9609-485a-4ed4-bf16-d5cc46c71024 https://atlas-auth.web.cern.ch/ https://arc1.example.com compute.modify * authtokens = 750e9609-485a-4ed4-bf16-d5cc46c71024 https://atlas-auth.web.cern.ch/ https://arc1.example.com compute.cancel * [authgroup: atlas_iam_sgm] authtokens = 5c5d2a4d-9177-3efa-912f-1b4e5c9fb660 https://atlas-auth.web.cern.ch/ https://arc1.example.com compute.create * authtokens = 5c5d2a4d-9177-3efa-912f-1b4e5c9fb660 https://atlas-auth.web.cern.ch/ https://arc1.example.com compute.read * authtokens = 5c5d2a4d-9177-3efa-912f-1b4e5c9fb660 https://atlas-auth.web.cern.ch/ https://arc1.example.com compute.modify * authtokens = 5c5d2a4d-9177-3efa-912f-1b4e5c9fb660 https://atlas-auth.web.cern.ch/ https://arc1.example.com compute.cancel * # again, just an example for ARC-CE running on arc1.example.com # (this is not the official CMS site configuration documentation) [authgroup: cms_iam_pilot] authtokens = bad55f4e-602c-4e8d-a5c5-bd8ffb762113 https://cms-auth.web.cern.ch/ https://arc1.example.com compute.create * authtokens = bad55f4e-602c-4e8d-a5c5-bd8ffb762113 https://cms-auth.web.cern.ch/ https://arc1.example.com compute.read * authtokens = bad55f4e-602c-4e8d-a5c5-bd8ffb762113 https://cms-auth.web.cern.ch/ https://arc1.example.com compute.modify * authtokens = bad55f4e-602c-4e8d-a5c5-bd8ffb762113 https://cms-auth.web.cern.ch/ https://arc1.example.com compute.cancel * [authgroup: cms_iam_test] authtokens = 08ca855e-d715-410e-a6ff-ad77306e1763 https://cms-auth.web.cern.ch/ https://arc1.example.com compute.create * authtokens = 08ca855e-d715-410e-a6ff-ad77306e1763 https://cms-auth.web.cern.ch/ https://arc1.example.com compute.read * authtokens = 08ca855e-d715-410e-a6ff-ad77306e1763 https://cms-auth.web.cern.ch/ https://arc1.example.com compute.modify * authtokens = 08ca855e-d715-410e-a6ff-ad77306e1763 https://cms-auth.web.cern.ch/ https://arc1.example.com compute.cancel * [authgroup: cms_iam_itb] authtokens = 490a9a36-0268-4070-8813-65af031be5a3 https://cms-auth.web.cern.ch/ https://arc1.example.com compute.create * authtokens = 490a9a36-0268-4070-8813-65af031be5a3 https://cms-auth.web.cern.ch/ https://arc1.example.com compute.read * authtokens = 490a9a36-0268-4070-8813-65af031be5a3 https://cms-auth.web.cern.ch/ https://arc1.example.com compute.modify * authtokens = 490a9a36-0268-4070-8813-65af031be5a3 https://cms-auth.web.cern.ch/ https://arc1.example.com compute.cancel * # this assumes existence of local users wlcg, egi, atlasprd, atlasplt, atlassgm, cmsplt, cmstest and cmsitb with corresponding groups [mapping] map_to_user = wlcg_iam wlcg:wlcg map_to_user = egi_aai egi:egi map_to_user = atlas_iam_prd atlasprd:atlasprd map_to_user = atlas_iam_plt atlasplt:atlasplt map_to_user = atlas_iam_sgm atlassgm:atlassgm map_to_user = cms_iam_pilot cmsplt:cmsplt map_to_user = cms_iam_test cmstest:cmstest map_to_user = cms_iam_itb cmsitb:cmsitb policy_on_nomap=stop [arex/ws/jobs] allowaccess=wlcg_iam allowaccess=egi_aai allowaccess=atlas_iam_prd allowaccess=atlas_iam_plt allowaccess=atlas_iam_sgm allowaccess=cms_iam_pilot allowaccess=cms_iam_test allowaccess=cms_iam_itb # ... Token implementation in ARC-CE is still preliminary and it seems possible there will be changes even in the structure of configuration file. You should follow official documentation if you install more recent ARC-CE version. Even though job submission is possible with tokens it is still necessary to have existing X.509 proxy, because current arcsub version still unconditionally verify proxy presence. On the other hand this proxy is delegated to ARC-CE regardless of auth mechanism used to submit jobs. With token in BEARER_TOKEN environement variable ( ARC_OTOKEN for older ARC-CE 6.7 clients) token'll be automatically used for authorization with HTTP interface, e.g. cat > test.xrsl <<EOF &(executable=\"/usr/bin/env\") (stdout=\"test.out\") (stderr=\"test.err\") (jobname=\"ARC-CE test\") (runtimeenvironment = \"ENV/PROXY\") EOF # following line is necessary for ARC-CE 6.7 client # newer version use BEARER_TOKEN or token discovery #export ARC_OTOKEN=$(oidc-token --aud=https://arc1.farm.particle.cz --scope=compute.read --scope=compute.modify --scope=compute.cancel --scope=compute.create --scope=wlcg.groups:/wlcg/pilots OIDC_STORE_NAME) export BEARER_OTOKEN=$(oidc-token --aud=https://arc1.farm.particle.cz --scope=compute.read --scope=compute.modify --scope=compute.cancel --scope=compute.create --scope=wlcg.groups:/wlcg/pilots OIDC_STORE_NAME) arcsub --debug=DEBUG --info-endpoint-type arcrest --submission-endpoint-type arcrest --computing-element arc1.farm.particle.cz test.xrsl ARC-CE support token discovery as described in WLCG Bearer Token Discovery Configured endpoints Site Host VO Audience Group praguelcg2 arc1.farm.particle.cz, arc2.farm.particle.cz wlcg * /wlcg/pilots praguelcg2 arc1.farm.particle.cz, arc2.farm.particle.cz atlas, dune * * HTCondor-CE It is possible to submit jobs with WLCG JWT token if HTCondor accepts SCITOKENS . By default HTCondor-CE is configured with non-standard aud claim , but this can be changed by adding following line in /etc/condor-ce/config.d/99-local file SCITOKENS_SERVER_AUDIENCE = $(SCITOKENS_SERVER_AUDIENCE) condor://$(COLLECTOR_HOST) # special \"any\" audience, not recommended for production use #SCITOKENS_SERVER_AUDIENCE = $(SCITOKENS_SERVER_AUDIENCE) https://wlcg.cern.ch/jwt/v1/any All other configuration and identity mapping is described in the documentation . Currently identity mapping is limited to sub + iss which is not sufficient for IAM shared by multiple groups (e.g. Fermialab), but there are plans to implement callout similar to the GSI lcmaps. Follow these steps to submit job with token to HTCondor-CE cat > test.sub <<EOF executable = /bin/env output = env.out error = env.err log = env.log queue EOF # get access token, e.g. from oidc-agent (see bellow) and store its # content in a file used by WLCG bearer token discovery mechanism oidc-token --aud=condor://osgce3.farm.particle.cz:9619 -s compute.create -s compute.read -s compute.modify -s compute.cancel wlcg-compute > $XDG_RUNTIME_DIR/bt_u$(id -u) export CONDOR_CONFIG=/dev/null export GSI_AUTHZ_CONF=/dev/null export _condor_AUTH_SSL_CLIENT_CADIR=/etc/grid-security/certificates export _condor_SEC_CLIENT_AUTHENTICATION_METHODS=SCITOKENS condor_ping -verbose -pool osgce3.farm.particle.cz:9619 -name osgce3.farm.particle.cz WRITE condor_submit -pool osgce3.farm.particle.cz:9619 -remote osgce3.farm.particle.cz test.sub condor_q -pool osgce3.farm.particle.cz:9619 -name osgce3.farm.particle.cz Obtain access token with compute.* scopes oidc-agent # start oidc-agent (if it is not already running) eval $(oidc-gen) # register new client in oidc-agent with compute.* scopes (one time # action, but not all IAM allows user to apply for compute.* scopes) oidc-gen --iss=https://wlcg.cloud.cnaf.infn.it/ --scope=\"openid offline_access wlcg.groups compute.create compute.read compute.modify compute.cancel\" wlcg-compute # obtain token with all necessary scopes and right audience oidc-token --aud=condor://osgce3.farm.particle.cz:9619 --scope \"compute.create compute.read compute.modify compute.cancel\" wlcg-compute > $XDG_RUNTIME_DIR/bt_u$(id -u) client credentials grant (client registered in IAM with compute.* scopes, grant type \"client credentials\" and response type \"token\") curl -s --data \"grant_type=client_credentials&client_id=client_id_from_iam_registration&client_secret=client_secret_from_iam_registration&audience=condor://osgce3.farm.particle.cz:9619\" https://wlcg.cloud.cnaf.infn.it/token | jq -r .access_token > $XDG_RUNTIME_DIR/bt_u$(id -u) Configured endpoints All OSG sites should now support job submission with token. You can test your HTCondor-CE configuration with ATLAS jobs submission. Site Host VO Audience osg hosted-ce-chtc-ubuntu.osg.chtc.io:9619 wlcg hosted-ce-chtc-ubuntu.osg.chtc.io:9619 praguelcg2 osgce1.farm.particle.cz:9619 wlcg, atlas, dune osgce1.farm.particle.cz:9619, condor://osgce1.farm.particle.cz:9619, https://wlcg.cern.ch/jwt/v1/any Storage Implementations dCache ( transfer documentation ) DPM Echo (CEPH + XRootD plugin) EOS StoRM ( configuration instructions ) XRootD ( configuration instructions ) Configured endpoints: Site Implementation Host VO Audience praguelcg2 DPM 1.15.0 https://golias100.farm.partile.cz/dpm/farm.particle.cz/home/wlcg wlcg https://wlcg.cern.ch/jwt/v1/any UNL XRootD https://red-gridftp12.unl.edu:1094/user/dteam wlcg DESY Devel dCache 7.1 https://prometheus.desy.de:2443/VOs/wlcg wlcg CNAF Prod StoRM https://xfer.cr.cnaf.infn.it:8443/wlcg wlcg CNAF Devel StoRM https://amnesiac.cloud.cnaf.infn.it:8443/wlcg wlcg CERN ATLAS EOS https://eosatlas.cern.ch:443/eos/atlas/atlasscratchdisk/3rdpartycopy atlas CERN Devel EOS https://eospps.cern.ch:443/eos/opstest/tpc/https wlcg RAL Echo https://ceph-gw8.gridpp.rl.ac.uk:1094/dteam:test/ wlcg Manchester Test DPM https://vm33.in.tier2.hep.manchester.ac.uk:443/dpm/tier2.hep.manchester.ac.uk/home/wlcg wlcg FTS CERN Devel FTS with 3.10.x provides JWT support for WLCG and XDC. fts-rest-whoami --access-token=<token> -s https://fts3-devel.cern.ch:8446 fts-rest-transfer-submit --access-token=<token> -s https://fts3-devel.cern.ch:8446 <src_url> <dst_url> fts-rest-transfer-status --access-token=<token> -s https://fts3-devel.cern.ch:8446 Be aware that for sucessfull FTS transfer submission with OIDC you also need recent 3.10.x FTS rest client. RUCIO Install Rucio client with one method described in documentation , e.g. # latest rucio client works only with python3 virtualenv-3 rucio source rucio/bin/activate pip3 install rucio-clients pip3 install gfal2-python and save following configuration file in rucio/etc/rucio.cfg for WLCG DOMA Rucio OIDC tests [client] rucio_host = https://rucio-doma.cern.ch:443 auth_host = https://rucio-doma-auth.cern.ch:443 auth_type = oidc account = wlcg_doma oidc_issuer = wlcg ca_cert = /etc/grid-security/certificates #ca_cert = /etc/pki/tls/certs/CERN-bundle.pem Setup environment for Rucio client installed in virtualenv source rucio/bin/activate or if you use different installation method just set RUCIO_HOME environment variable to the base directory with etc/rucio.cfg file export RUCIO_HOME=/base/path/to/rucio WLCG IAM account is necessary to access WLCG DOMA Rucio instance and user sub claim (WLCG IAM uuid identity) must be associated with wlcg_doma Rucio account by DOMA Rucio ADMIN. It is also possible to associate user certificate subject with wlcg_doma Rucio account to provide access with WLCG VO X.509 certificate proxy, but for different authorization type it is necessary to update auth_type = x509_proxy in rucio.cfg or setting environment variable RUCIO_AUTH_TYPE=x509_proxy . Monitoring Functional Tests with WLCG OIDC Rucio DOMA Configuration Rucio DOMA Logs CERN FTS3 Devel FTS Grafana","title":"DOMA Token-based AuthZ Testbed"},{"location":"token-based-authorization/doma-testbed/#doma-token-based-authz-testbed","text":"","title":"DOMA Token-based AuthZ Testbed"},{"location":"token-based-authorization/doma-testbed/#joining-testbed","text":"Send mail to wlcg-doma-tpc e-group if you want to join Rucio DOMA Functional Tests OIDC or advertise your CE that support job submission with tokens. Please include necessary to access your SE/CE endpoint, as an example you can use entries that were already filled in tables with available resources that to some degree already supports tokens.","title":"Joining testbed"},{"location":"token-based-authorization/doma-testbed/#iam","text":"To be able to use tokens it is necessary to know which token issuer is used by individual VOs. For tests we use WLCG IAM token issuer which provides services for \"wlcg\" VO, e.g. VO Issuer WLCG IAM https://wlcg.cloud.cnaf.infn.it/ ATLAS IAM https://atlas-auth.web.cern.ch/ CMS IAM https://cms-auth.web.cern.ch/ DUNE https://cilogon.org/dune Fermilab https://cilogon.org/fermilab OSG have token isser details stored with all VO detail in their virtual-organization topology files. WLCG/EGI currently doesn't provide common (trusted) place with token isser details, but naturally this could become part of EGI VO ID Cards (briefly touched this topic here ).","title":"IAM"},{"location":"token-based-authorization/doma-testbed/#ce","text":"","title":"CE"},{"location":"token-based-authorization/doma-testbed/#arc-ce","text":"ARC-CE 6.12 still has limited support for WLCG JWT tokens and they can be only used to submit jobs with HTTP based protocols ( EMI-ES and REST interface). With current limitations configuration close to expectations from WLCG JWT profile could look like (replace arc1.example.com with your ARC-CE hostname or arc1.example.com:port_number if you don't use standard HTTPS port 443 ): # ... [authtokens] [authgroup: wlcg_iam] # capability based authorization that use compute.* scopes authtokens = * https://wlcg.cloud.cnaf.infn.it/ https://arc1.example.com compute.create * authtokens = * https://wlcg.cloud.cnaf.infn.it/ https://arc1.example.com compute.read * authtokens = * https://wlcg.cloud.cnaf.infn.it/ https://arc1.example.com compute.modify * authtokens = * https://wlcg.cloud.cnaf.infn.it/ https://arc1.example.com compute.cancel * # group based authorization that use /wlcg/pilots group (LHC experiments prefer capabilities) authtokens = * https://wlcg.cloud.cnaf.infn.it/ https://arc1.example.com * /wlcg/pilots # accept token issued by EGI Check-in for job submission (both old MitreID and new Keycloak issuer) [authgroup: egi_aai] # very rough / DANGEROUS configuration - accepting all tokens without restrictions #authtokens = * https://aai.egi.eu/oidc/ * * * #authtokens = * https://aai.egi.eu/auth/realms/egi * * * # it is possible to restrict job submission to the specific EGI user authtokens = 85ff127e07ea6660c727831b99e18e4e96b319283f8d2cc8113f405bad2ba261@egi.eu https://aai.egi.eu/oidc/ * * * authtokens = 85ff127e07ea6660c727831b99e18e4e96b319283f8d2cc8113f405bad2ba261@egi.eu https://aai.egi.eu/auth/realms/egi * * * # just an example for ARC-CE running on arc1.example.com # recommendation for ATLAS configuration may change in fugure # (this is not the official ATLAS site configuration documentation) [authgroup: atlas_iam_prd] authtokens = 7dee38a3-6ab8-4fe2-9e4c-58039c21d817 https://atlas-auth.web.cern.ch/ https://arc1.example.com compute.create * authtokens = 7dee38a3-6ab8-4fe2-9e4c-58039c21d817 https://atlas-auth.web.cern.ch/ https://arc1.example.com compute.read * authtokens = 7dee38a3-6ab8-4fe2-9e4c-58039c21d817 https://atlas-auth.web.cern.ch/ https://arc1.example.com compute.modify * authtokens = 7dee38a3-6ab8-4fe2-9e4c-58039c21d817 https://atlas-auth.web.cern.ch/ https://arc1.example.com compute.cancel * [authgroup: atlas_iam_plt] authtokens = 750e9609-485a-4ed4-bf16-d5cc46c71024 https://atlas-auth.web.cern.ch/ https://arc1.example.com compute.create * authtokens = 750e9609-485a-4ed4-bf16-d5cc46c71024 https://atlas-auth.web.cern.ch/ https://arc1.example.com compute.read * authtokens = 750e9609-485a-4ed4-bf16-d5cc46c71024 https://atlas-auth.web.cern.ch/ https://arc1.example.com compute.modify * authtokens = 750e9609-485a-4ed4-bf16-d5cc46c71024 https://atlas-auth.web.cern.ch/ https://arc1.example.com compute.cancel * [authgroup: atlas_iam_sgm] authtokens = 5c5d2a4d-9177-3efa-912f-1b4e5c9fb660 https://atlas-auth.web.cern.ch/ https://arc1.example.com compute.create * authtokens = 5c5d2a4d-9177-3efa-912f-1b4e5c9fb660 https://atlas-auth.web.cern.ch/ https://arc1.example.com compute.read * authtokens = 5c5d2a4d-9177-3efa-912f-1b4e5c9fb660 https://atlas-auth.web.cern.ch/ https://arc1.example.com compute.modify * authtokens = 5c5d2a4d-9177-3efa-912f-1b4e5c9fb660 https://atlas-auth.web.cern.ch/ https://arc1.example.com compute.cancel * # again, just an example for ARC-CE running on arc1.example.com # (this is not the official CMS site configuration documentation) [authgroup: cms_iam_pilot] authtokens = bad55f4e-602c-4e8d-a5c5-bd8ffb762113 https://cms-auth.web.cern.ch/ https://arc1.example.com compute.create * authtokens = bad55f4e-602c-4e8d-a5c5-bd8ffb762113 https://cms-auth.web.cern.ch/ https://arc1.example.com compute.read * authtokens = bad55f4e-602c-4e8d-a5c5-bd8ffb762113 https://cms-auth.web.cern.ch/ https://arc1.example.com compute.modify * authtokens = bad55f4e-602c-4e8d-a5c5-bd8ffb762113 https://cms-auth.web.cern.ch/ https://arc1.example.com compute.cancel * [authgroup: cms_iam_test] authtokens = 08ca855e-d715-410e-a6ff-ad77306e1763 https://cms-auth.web.cern.ch/ https://arc1.example.com compute.create * authtokens = 08ca855e-d715-410e-a6ff-ad77306e1763 https://cms-auth.web.cern.ch/ https://arc1.example.com compute.read * authtokens = 08ca855e-d715-410e-a6ff-ad77306e1763 https://cms-auth.web.cern.ch/ https://arc1.example.com compute.modify * authtokens = 08ca855e-d715-410e-a6ff-ad77306e1763 https://cms-auth.web.cern.ch/ https://arc1.example.com compute.cancel * [authgroup: cms_iam_itb] authtokens = 490a9a36-0268-4070-8813-65af031be5a3 https://cms-auth.web.cern.ch/ https://arc1.example.com compute.create * authtokens = 490a9a36-0268-4070-8813-65af031be5a3 https://cms-auth.web.cern.ch/ https://arc1.example.com compute.read * authtokens = 490a9a36-0268-4070-8813-65af031be5a3 https://cms-auth.web.cern.ch/ https://arc1.example.com compute.modify * authtokens = 490a9a36-0268-4070-8813-65af031be5a3 https://cms-auth.web.cern.ch/ https://arc1.example.com compute.cancel * # this assumes existence of local users wlcg, egi, atlasprd, atlasplt, atlassgm, cmsplt, cmstest and cmsitb with corresponding groups [mapping] map_to_user = wlcg_iam wlcg:wlcg map_to_user = egi_aai egi:egi map_to_user = atlas_iam_prd atlasprd:atlasprd map_to_user = atlas_iam_plt atlasplt:atlasplt map_to_user = atlas_iam_sgm atlassgm:atlassgm map_to_user = cms_iam_pilot cmsplt:cmsplt map_to_user = cms_iam_test cmstest:cmstest map_to_user = cms_iam_itb cmsitb:cmsitb policy_on_nomap=stop [arex/ws/jobs] allowaccess=wlcg_iam allowaccess=egi_aai allowaccess=atlas_iam_prd allowaccess=atlas_iam_plt allowaccess=atlas_iam_sgm allowaccess=cms_iam_pilot allowaccess=cms_iam_test allowaccess=cms_iam_itb # ... Token implementation in ARC-CE is still preliminary and it seems possible there will be changes even in the structure of configuration file. You should follow official documentation if you install more recent ARC-CE version. Even though job submission is possible with tokens it is still necessary to have existing X.509 proxy, because current arcsub version still unconditionally verify proxy presence. On the other hand this proxy is delegated to ARC-CE regardless of auth mechanism used to submit jobs. With token in BEARER_TOKEN environement variable ( ARC_OTOKEN for older ARC-CE 6.7 clients) token'll be automatically used for authorization with HTTP interface, e.g. cat > test.xrsl <<EOF &(executable=\"/usr/bin/env\") (stdout=\"test.out\") (stderr=\"test.err\") (jobname=\"ARC-CE test\") (runtimeenvironment = \"ENV/PROXY\") EOF # following line is necessary for ARC-CE 6.7 client # newer version use BEARER_TOKEN or token discovery #export ARC_OTOKEN=$(oidc-token --aud=https://arc1.farm.particle.cz --scope=compute.read --scope=compute.modify --scope=compute.cancel --scope=compute.create --scope=wlcg.groups:/wlcg/pilots OIDC_STORE_NAME) export BEARER_OTOKEN=$(oidc-token --aud=https://arc1.farm.particle.cz --scope=compute.read --scope=compute.modify --scope=compute.cancel --scope=compute.create --scope=wlcg.groups:/wlcg/pilots OIDC_STORE_NAME) arcsub --debug=DEBUG --info-endpoint-type arcrest --submission-endpoint-type arcrest --computing-element arc1.farm.particle.cz test.xrsl ARC-CE support token discovery as described in WLCG Bearer Token Discovery","title":"ARC-CE"},{"location":"token-based-authorization/doma-testbed/#configured-endpoints","text":"Site Host VO Audience Group praguelcg2 arc1.farm.particle.cz, arc2.farm.particle.cz wlcg * /wlcg/pilots praguelcg2 arc1.farm.particle.cz, arc2.farm.particle.cz atlas, dune * *","title":"Configured endpoints"},{"location":"token-based-authorization/doma-testbed/#htcondor-ce","text":"It is possible to submit jobs with WLCG JWT token if HTCondor accepts SCITOKENS . By default HTCondor-CE is configured with non-standard aud claim , but this can be changed by adding following line in /etc/condor-ce/config.d/99-local file SCITOKENS_SERVER_AUDIENCE = $(SCITOKENS_SERVER_AUDIENCE) condor://$(COLLECTOR_HOST) # special \"any\" audience, not recommended for production use #SCITOKENS_SERVER_AUDIENCE = $(SCITOKENS_SERVER_AUDIENCE) https://wlcg.cern.ch/jwt/v1/any All other configuration and identity mapping is described in the documentation . Currently identity mapping is limited to sub + iss which is not sufficient for IAM shared by multiple groups (e.g. Fermialab), but there are plans to implement callout similar to the GSI lcmaps. Follow these steps to submit job with token to HTCondor-CE cat > test.sub <<EOF executable = /bin/env output = env.out error = env.err log = env.log queue EOF # get access token, e.g. from oidc-agent (see bellow) and store its # content in a file used by WLCG bearer token discovery mechanism oidc-token --aud=condor://osgce3.farm.particle.cz:9619 -s compute.create -s compute.read -s compute.modify -s compute.cancel wlcg-compute > $XDG_RUNTIME_DIR/bt_u$(id -u) export CONDOR_CONFIG=/dev/null export GSI_AUTHZ_CONF=/dev/null export _condor_AUTH_SSL_CLIENT_CADIR=/etc/grid-security/certificates export _condor_SEC_CLIENT_AUTHENTICATION_METHODS=SCITOKENS condor_ping -verbose -pool osgce3.farm.particle.cz:9619 -name osgce3.farm.particle.cz WRITE condor_submit -pool osgce3.farm.particle.cz:9619 -remote osgce3.farm.particle.cz test.sub condor_q -pool osgce3.farm.particle.cz:9619 -name osgce3.farm.particle.cz","title":"HTCondor-CE"},{"location":"token-based-authorization/doma-testbed/#obtain-access-token-with-compute-scopes","text":"oidc-agent # start oidc-agent (if it is not already running) eval $(oidc-gen) # register new client in oidc-agent with compute.* scopes (one time # action, but not all IAM allows user to apply for compute.* scopes) oidc-gen --iss=https://wlcg.cloud.cnaf.infn.it/ --scope=\"openid offline_access wlcg.groups compute.create compute.read compute.modify compute.cancel\" wlcg-compute # obtain token with all necessary scopes and right audience oidc-token --aud=condor://osgce3.farm.particle.cz:9619 --scope \"compute.create compute.read compute.modify compute.cancel\" wlcg-compute > $XDG_RUNTIME_DIR/bt_u$(id -u) client credentials grant (client registered in IAM with compute.* scopes, grant type \"client credentials\" and response type \"token\") curl -s --data \"grant_type=client_credentials&client_id=client_id_from_iam_registration&client_secret=client_secret_from_iam_registration&audience=condor://osgce3.farm.particle.cz:9619\" https://wlcg.cloud.cnaf.infn.it/token | jq -r .access_token > $XDG_RUNTIME_DIR/bt_u$(id -u)","title":"Obtain access token with compute.* scopes"},{"location":"token-based-authorization/doma-testbed/#configured-endpoints_1","text":"All OSG sites should now support job submission with token. You can test your HTCondor-CE configuration with ATLAS jobs submission. Site Host VO Audience osg hosted-ce-chtc-ubuntu.osg.chtc.io:9619 wlcg hosted-ce-chtc-ubuntu.osg.chtc.io:9619 praguelcg2 osgce1.farm.particle.cz:9619 wlcg, atlas, dune osgce1.farm.particle.cz:9619, condor://osgce1.farm.particle.cz:9619, https://wlcg.cern.ch/jwt/v1/any","title":"Configured endpoints"},{"location":"token-based-authorization/doma-testbed/#storage","text":"","title":"Storage"},{"location":"token-based-authorization/doma-testbed/#implementations","text":"dCache ( transfer documentation ) DPM Echo (CEPH + XRootD plugin) EOS StoRM ( configuration instructions ) XRootD ( configuration instructions )","title":"Implementations"},{"location":"token-based-authorization/doma-testbed/#configured-endpoints_2","text":"Site Implementation Host VO Audience praguelcg2 DPM 1.15.0 https://golias100.farm.partile.cz/dpm/farm.particle.cz/home/wlcg wlcg https://wlcg.cern.ch/jwt/v1/any UNL XRootD https://red-gridftp12.unl.edu:1094/user/dteam wlcg DESY Devel dCache 7.1 https://prometheus.desy.de:2443/VOs/wlcg wlcg CNAF Prod StoRM https://xfer.cr.cnaf.infn.it:8443/wlcg wlcg CNAF Devel StoRM https://amnesiac.cloud.cnaf.infn.it:8443/wlcg wlcg CERN ATLAS EOS https://eosatlas.cern.ch:443/eos/atlas/atlasscratchdisk/3rdpartycopy atlas CERN Devel EOS https://eospps.cern.ch:443/eos/opstest/tpc/https wlcg RAL Echo https://ceph-gw8.gridpp.rl.ac.uk:1094/dteam:test/ wlcg Manchester Test DPM https://vm33.in.tier2.hep.manchester.ac.uk:443/dpm/tier2.hep.manchester.ac.uk/home/wlcg wlcg","title":"Configured endpoints:"},{"location":"token-based-authorization/doma-testbed/#fts","text":"CERN Devel FTS with 3.10.x provides JWT support for WLCG and XDC. fts-rest-whoami --access-token=<token> -s https://fts3-devel.cern.ch:8446 fts-rest-transfer-submit --access-token=<token> -s https://fts3-devel.cern.ch:8446 <src_url> <dst_url> fts-rest-transfer-status --access-token=<token> -s https://fts3-devel.cern.ch:8446 Be aware that for sucessfull FTS transfer submission with OIDC you also need recent 3.10.x FTS rest client.","title":"FTS"},{"location":"token-based-authorization/doma-testbed/#rucio","text":"Install Rucio client with one method described in documentation , e.g. # latest rucio client works only with python3 virtualenv-3 rucio source rucio/bin/activate pip3 install rucio-clients pip3 install gfal2-python and save following configuration file in rucio/etc/rucio.cfg for WLCG DOMA Rucio OIDC tests [client] rucio_host = https://rucio-doma.cern.ch:443 auth_host = https://rucio-doma-auth.cern.ch:443 auth_type = oidc account = wlcg_doma oidc_issuer = wlcg ca_cert = /etc/grid-security/certificates #ca_cert = /etc/pki/tls/certs/CERN-bundle.pem Setup environment for Rucio client installed in virtualenv source rucio/bin/activate or if you use different installation method just set RUCIO_HOME environment variable to the base directory with etc/rucio.cfg file export RUCIO_HOME=/base/path/to/rucio WLCG IAM account is necessary to access WLCG DOMA Rucio instance and user sub claim (WLCG IAM uuid identity) must be associated with wlcg_doma Rucio account by DOMA Rucio ADMIN. It is also possible to associate user certificate subject with wlcg_doma Rucio account to provide access with WLCG VO X.509 certificate proxy, but for different authorization type it is necessary to update auth_type = x509_proxy in rucio.cfg or setting environment variable RUCIO_AUTH_TYPE=x509_proxy .","title":"RUCIO"},{"location":"token-based-authorization/doma-testbed/#monitoring","text":"Functional Tests with WLCG OIDC Rucio DOMA Configuration Rucio DOMA Logs CERN FTS3 Devel FTS Grafana","title":"Monitoring"},{"location":"token-based-authorization/issues/","text":"Known issues oidc-agent not available in EPEL repository Scitokens storage.modify scope is not honoured by scitokens-cpp : Small, but breaking issue since the scitokens library honours the storage.write instead of the storage.modify scope. Breaks WLCG JWT profile compatibility for any software that relies on the library. XRootD StoRM StoRM WebDAV should drop Authorization header in TPC redirects : this is solved and coming with the next StoRM release scheduled for the beginning of July; StoRM WebDAV leaks file descriptors when Conscrypt is enabled : This is fixed in the code; a workaround to avoid this issue is disabling Conscrypt. This can be done by setting the following variables in /etc/sysconfig/storm-webdav : STORM_WEBDAV_USE_CONSCRYPT=\"false\" STORM_WEBDAV_TPC_USE_CONSCRYPT=\"false\" STORM_WEBDAV_ENABLE_HTTP2=\"false\" dCache DPM allows HEAD requests with just storage.create:/ (mapping internally capabilities to simple ro/rw activity) supports storage.write scope which is not defined in WLCG JWT profile insecure token identity mapping to local uid (only \"sub\" claim considered while only \"sub\"+\"iss\" guaranteed to be unique) EOS RUCIO token with rucio audience used for FTS file transfers (changed to https://wlcg.cern.ch/jwt/v1/any for Rucio DOMA instnace) default scope set to openid profile wich is not sufficient e.g. for rucio upload or rucio download fails in case there is expired X.509 proxy in /tmp/x509up_u$(id -u) although auth_type is set to oidc use non-standard location for access token /tmp/$(id -u -n)/.rucio_${RUCIO_ACCOUNT}/auth_token_${RUCIO_ACCOUNT} CLI OIDC arguments differs from FTS REST client FTS Gfal tokens can't be passed to gfal-utils CLI tools unable to determine which audience and scope storage.*:/path should be in used for SRM+HTTP transfers (TURL returned by SRM srmPrepareToGet + srmPrepareToPut not known before calling copy function) TPC transfers with StoRM active party fails with gfal2 KEEP_ALIVE=True configuration and davs:// protocol (can be reproduced also by 2x ctx.stat('davs://storm:8443/non-existing-file') using gfal python API)","title":"Known issues"},{"location":"token-based-authorization/issues/#known-issues","text":"","title":"Known issues"},{"location":"token-based-authorization/issues/#oidc-agent","text":"not available in EPEL repository","title":"oidc-agent"},{"location":"token-based-authorization/issues/#scitokens","text":"storage.modify scope is not honoured by scitokens-cpp : Small, but breaking issue since the scitokens library honours the storage.write instead of the storage.modify scope. Breaks WLCG JWT profile compatibility for any software that relies on the library.","title":"Scitokens"},{"location":"token-based-authorization/issues/#xrootd","text":"","title":"XRootD"},{"location":"token-based-authorization/issues/#storm","text":"StoRM WebDAV should drop Authorization header in TPC redirects : this is solved and coming with the next StoRM release scheduled for the beginning of July; StoRM WebDAV leaks file descriptors when Conscrypt is enabled : This is fixed in the code; a workaround to avoid this issue is disabling Conscrypt. This can be done by setting the following variables in /etc/sysconfig/storm-webdav : STORM_WEBDAV_USE_CONSCRYPT=\"false\" STORM_WEBDAV_TPC_USE_CONSCRYPT=\"false\" STORM_WEBDAV_ENABLE_HTTP2=\"false\"","title":"StoRM"},{"location":"token-based-authorization/issues/#dcache","text":"","title":"dCache"},{"location":"token-based-authorization/issues/#dpm","text":"allows HEAD requests with just storage.create:/ (mapping internally capabilities to simple ro/rw activity) supports storage.write scope which is not defined in WLCG JWT profile insecure token identity mapping to local uid (only \"sub\" claim considered while only \"sub\"+\"iss\" guaranteed to be unique)","title":"DPM"},{"location":"token-based-authorization/issues/#eos","text":"","title":"EOS"},{"location":"token-based-authorization/issues/#rucio","text":"token with rucio audience used for FTS file transfers (changed to https://wlcg.cern.ch/jwt/v1/any for Rucio DOMA instnace) default scope set to openid profile wich is not sufficient e.g. for rucio upload or rucio download fails in case there is expired X.509 proxy in /tmp/x509up_u$(id -u) although auth_type is set to oidc use non-standard location for access token /tmp/$(id -u -n)/.rucio_${RUCIO_ACCOUNT}/auth_token_${RUCIO_ACCOUNT} CLI OIDC arguments differs from FTS REST client","title":"RUCIO"},{"location":"token-based-authorization/issues/#fts","text":"","title":"FTS"},{"location":"token-based-authorization/issues/#gfal","text":"tokens can't be passed to gfal-utils CLI tools unable to determine which audience and scope storage.*:/path should be in used for SRM+HTTP transfers (TURL returned by SRM srmPrepareToGet + srmPrepareToPut not known before calling copy function) TPC transfers with StoRM active party fails with gfal2 KEEP_ALIVE=True configuration and davs:// protocol (can be reproduced also by 2x ctx.stat('davs://storm:8443/non-existing-file') using gfal python API)","title":"Gfal"},{"location":"token-based-authorization/oidc-agent/","text":"OIDC Agent oidc-agent is a set of tools to get and manage OpenID Connect tokens and make them easily usable from the command line. It follows the ssh-agent design, so users can handle OIDC tokens in a similar way as they do with ssh keys. Installation instructions Releases are available for several RPM and DEB based distribution, plus MacOS and Windows. The KIT Repo Server shows installation instructions for each release This recipe shows how to quickly install oidc-agent on CENTOS 7 . yum install --repofrompath oidc,https://repo.data.kit.edu/centos/centos7 oidc-agent Bootstrapping oidc-agent The first thing to do is to start oidc-agent. This can be done issuing the following command: $ eval $(oidc-agent) Agent pid 62088 To install OIDC Agent from source or in a Debian/Ubuntu distro, please refer to the KIT Repo Server and to the oidc-agent installation documentation in gitbook How to register a client In order to obtain a token out of an OP such as IAM, a user needs a client registered. oidc-agent can automate this step and store client credentials securely on the user machine. A new client is registered using the oidc-gen command, as follows: $ oidc-gen --iss https://wlcg.cloud.cnaf.infn.it --scope max --flow=device wlcg The --flow=device instructs oidc-agent to use the device code flow for the authentication, which is the recommended way with IAM. oidc-agent will use \"dynamic client registration\" to register a new client and store the client credentials and a refresh token locally in encrypted form (the agent will ask for a password from the user). Some IAM instance may not have \"dynamic client registration\" enabled. In this case you may try oidc-gen with the --pub parameter. This makes use of a pre-registered \"public client\", which may be available for that IAM instance. How to print a list of all configured accounts To obtain a list of all configured accounts configured, either oidc-gen --accounts or oidc-add --list can be used. Both of them can use the same flag -l $ oidc-gen -l The following account configurations are usable: wlcg How to print a client description Printing the full client decrypted content can be done by passing the account shortname or the absolute filepath of the account, with oidc-gen --print or simply the -p flag $ oidc-gen -p wlcg Enter decryption password for account config 'wlcg': { \"name\": \"wlcg\", \"client_name\": \"oidc-agent:wlcg\", \"issuer_url\": \"https://wlcg.cloud.cnaf.infn.it/\", \"device_authorization_endpoint\": \"https://wlcg.cloud.cnaf.infn.it/devicecode\", \"daeSetByUser\": 0, \"client_id\": \"f062c71e-920d-4b64-8282-a24d4101d8fc\", \"client_secret\": \"xxxxxxxxxxxxxxxx\", \"refresh_token\": \"xxxxxxxxxxxxxxxx\", \"cert_path\": \"\", \"scope\": \"address openid profile storage.read:/ wlcg eduperson_entitlement storage.create:/ phone offline_access eduperson_scoped_affiliation storage.modify:/ email wlcg.groups\", \"audience\": \"\", \"redirect_uris\": [\"edu.kit.data.oidc-agent:/redirect\", \"http://localhost:8080\", \"http://localhost:4242\", \"http://localhost:10088\"], \"username\": \"\", \"password\": \"\", \"client_id_issued_at\": 1592322007, \"registration_access_token\": \"xxxxxxxxxxxxxx\", \"registration_client_uri\": \"xxxxxxxxxxxxxx\", \"token_endpoint_auth_method\": \"client_secret_basic\", \"grant_types\": [\"urn:ietf:params:oauth:grant-type:device_code\", \"refresh_token\"], \"response_types\": [\"token\"], \"application_type\": \"web\", \"cert_path\": \"\", \"audience\": \"\" } How to get a token from oidc-agent Tokens can be obtained using the oidc-token command, as follows: oidc-token wlcg This will request a token with all the scopes requested at client registration time. Limiting issued scopes To limit the scopes included in the token, the -s flag can be used, as follows: oidc-token -s storage.read:/ wlcg In this example the scopes is being limited to storage.read:/ Limiting token audience The token audience can be limited using the --aud flag, oidc-token --aud example.audience -s storage.read:/ wlcg In this example the audience is being defined as example.audience For more usage options please refer to oidc-agent --help or to oidc-agent usage documentation","title":"OIDC Agent"},{"location":"token-based-authorization/oidc-agent/#oidc-agent","text":"oidc-agent is a set of tools to get and manage OpenID Connect tokens and make them easily usable from the command line. It follows the ssh-agent design, so users can handle OIDC tokens in a similar way as they do with ssh keys.","title":"OIDC Agent"},{"location":"token-based-authorization/oidc-agent/#installation-instructions","text":"Releases are available for several RPM and DEB based distribution, plus MacOS and Windows. The KIT Repo Server shows installation instructions for each release This recipe shows how to quickly install oidc-agent on CENTOS 7 . yum install --repofrompath oidc,https://repo.data.kit.edu/centos/centos7 oidc-agent","title":"Installation instructions"},{"location":"token-based-authorization/oidc-agent/#bootstrapping-oidc-agent","text":"The first thing to do is to start oidc-agent. This can be done issuing the following command: $ eval $(oidc-agent) Agent pid 62088 To install OIDC Agent from source or in a Debian/Ubuntu distro, please refer to the KIT Repo Server and to the oidc-agent installation documentation in gitbook","title":"Bootstrapping oidc-agent"},{"location":"token-based-authorization/oidc-agent/#how-to-register-a-client","text":"In order to obtain a token out of an OP such as IAM, a user needs a client registered. oidc-agent can automate this step and store client credentials securely on the user machine. A new client is registered using the oidc-gen command, as follows: $ oidc-gen --iss https://wlcg.cloud.cnaf.infn.it --scope max --flow=device wlcg The --flow=device instructs oidc-agent to use the device code flow for the authentication, which is the recommended way with IAM. oidc-agent will use \"dynamic client registration\" to register a new client and store the client credentials and a refresh token locally in encrypted form (the agent will ask for a password from the user). Some IAM instance may not have \"dynamic client registration\" enabled. In this case you may try oidc-gen with the --pub parameter. This makes use of a pre-registered \"public client\", which may be available for that IAM instance.","title":"How to register a client"},{"location":"token-based-authorization/oidc-agent/#how-to-print-a-list-of-all-configured-accounts","text":"To obtain a list of all configured accounts configured, either oidc-gen --accounts or oidc-add --list can be used. Both of them can use the same flag -l $ oidc-gen -l The following account configurations are usable: wlcg","title":"How to print a list of all configured accounts"},{"location":"token-based-authorization/oidc-agent/#how-to-print-a-client-description","text":"Printing the full client decrypted content can be done by passing the account shortname or the absolute filepath of the account, with oidc-gen --print or simply the -p flag $ oidc-gen -p wlcg Enter decryption password for account config 'wlcg': { \"name\": \"wlcg\", \"client_name\": \"oidc-agent:wlcg\", \"issuer_url\": \"https://wlcg.cloud.cnaf.infn.it/\", \"device_authorization_endpoint\": \"https://wlcg.cloud.cnaf.infn.it/devicecode\", \"daeSetByUser\": 0, \"client_id\": \"f062c71e-920d-4b64-8282-a24d4101d8fc\", \"client_secret\": \"xxxxxxxxxxxxxxxx\", \"refresh_token\": \"xxxxxxxxxxxxxxxx\", \"cert_path\": \"\", \"scope\": \"address openid profile storage.read:/ wlcg eduperson_entitlement storage.create:/ phone offline_access eduperson_scoped_affiliation storage.modify:/ email wlcg.groups\", \"audience\": \"\", \"redirect_uris\": [\"edu.kit.data.oidc-agent:/redirect\", \"http://localhost:8080\", \"http://localhost:4242\", \"http://localhost:10088\"], \"username\": \"\", \"password\": \"\", \"client_id_issued_at\": 1592322007, \"registration_access_token\": \"xxxxxxxxxxxxxx\", \"registration_client_uri\": \"xxxxxxxxxxxxxx\", \"token_endpoint_auth_method\": \"client_secret_basic\", \"grant_types\": [\"urn:ietf:params:oauth:grant-type:device_code\", \"refresh_token\"], \"response_types\": [\"token\"], \"application_type\": \"web\", \"cert_path\": \"\", \"audience\": \"\" }","title":"How to print a client description"},{"location":"token-based-authorization/oidc-agent/#how-to-get-a-token-from-oidc-agent","text":"Tokens can be obtained using the oidc-token command, as follows: oidc-token wlcg This will request a token with all the scopes requested at client registration time.","title":"How to get a token from oidc-agent"},{"location":"token-based-authorization/oidc-agent/#limiting-issued-scopes","text":"To limit the scopes included in the token, the -s flag can be used, as follows: oidc-token -s storage.read:/ wlcg In this example the scopes is being limited to storage.read:/","title":"Limiting issued scopes"},{"location":"token-based-authorization/oidc-agent/#limiting-token-audience","text":"The token audience can be limited using the --aud flag, oidc-token --aud example.audience -s storage.read:/ wlcg In this example the audience is being defined as example.audience For more usage options please refer to oidc-agent --help or to oidc-agent usage documentation","title":"Limiting token audience"},{"location":"token-based-authorization/configuration/dcache/","text":"dCache Official documentation currently doesn't provide all necessary information how to configured storage to accept and map WLCG JWT tokens (see dCache#6607 ). dCache provides two different gPlazma plugins to deal with OIDC tokens - oidc and scitoken . With dCache 7.x it is necessary to use scitoken plugin, because this is the only way to deal with WLCG JWT tokens or SCITOKENS. The dCache 8.x comes with more generic oidc plugin that can process also WLCG JWT tokens and SCITOKENS ( scitoken plugin is still kept for compatibility reasons, but it is deprecated). dCache 7.2 configuration To accept connection authorized with WLCG JWT tokens it is necessary to add scitoken plugin in the chain of gPlazma auth plugins. Just add following line in your /etc/dcache/gplazma.conf configuration auth optional scitoken map sufficient multimap gplazma.multimap.file=/etc/dcache/multi-mapfile.wlcg_jwt This line alone is not sufficient for gPlazma service configuration, because it is necessary to specify supported WLCG JWT issuers. This can be done in the corresponding layout file with defined gPlazma service e.g. # ... [centralDomain/gplazma] # assuming that VO starts in top level directory gplazma.scitoken.issuer!wlcg = https://wlcg.cloud.cnaf.infn.it/ /wlcg gplazma.scitoken.issuer!altas = https://atlas-auth.web.cern.ch/ /atlas gplazma.scitoken.issuer!cms = https://cms-auth.web.cern.ch/ /cms # assuming that dCache WebDAV service runs on default HTTPS port 443 for doors dcache.example.com #gplazma.scitoken.audience-targets = https://dcache.example.com # you can specify multiple audiences, but avoid using generic https://wlcg.cern.ch/jwt/v1/any on production instances # (https://wlcg.cern.ch/jwt/v1/any is necessary for compliance testbed) gplazma.scitoken.audience-targets = https://wlcg.cern.ch/jwt/v1/any https://dcache.example.com https://dcache.example.com:2880 https://alias.example.com # ... It is necessary to map identity extracted from WLCG JWT token (in this case we rely on multimap \" op \" that gives us access to the token issuer mapped to our name configured in the layout file) to the dCache uid , gid and username , e.g. by using multimap file /etc/dcache/multi-mapfile.wlcg_jwt op:wlcg uid:1999 gid:1999,true username:wlcg_oidc op:atlas uid:2999 gid:2999,true username:atlas_oidc op:cms uid:3999 gid:3999,true username:cms_oidc Be wery careful how you map WLCG JWT token indentity and when you support also X.509 voms proxies. Most probably it'll be necessary to very carefully add additional ACLs to your VO (sub)directories. dCache 8.2 configuration FIXME: this configuration was not yet tested auth optional oidc map sufficient multimap gplazma.multimap.file=/etc/dcache/multi-mapfile.wlcg_jwt # ... [centralDomain/gplazma] # assuming that VO starts in top level directory gplazma.oidc.provider!wlcg = https://wlcg.cloud.cnaf.infn.it/ -profile=wlcg -prefix=/wlcg gplazma.oidc.provider!altas = https://atlas-auth.web.cern.ch/ -profile=wlcg -prefix=/atlas gplazma.oidc.provider!cms = https://cms-auth.web.cern.ch/ -profile=wlcg -prefix=/cms # assuming that dCache WebDAV service runs on default HTTPS port 443 for doors dcache.example.com #gplazma.oidc.audience-targets = https://dcache.example.com # you can specify multiple audiences, but avoid using generic https://wlcg.cern.ch/jwt/v1/any on production instances # (https://wlcg.cern.ch/jwt/v1/any is necessary for compliance testbed) gplazma.oidc.audience-targets = https://wlcg.cern.ch/jwt/v1/any https://dcache.example.com https://dcache.example.com:2880 https://alias.example.com # ... ACL configuration To be able to use ACLs with dCache it is necessary to enable them in the dcache.conf # enable ACL support pnfsmanager.enable.acl = true WLCG compliance testbed Storage configuration for compliance tests. ATLAS /atlas-root ... read only for /atlas group /atlas-root/atlasscratchdisk ... inheritable read for FQAN:/atlas and write for FQAN:/atlas /atlas-root/atlasdatadisk ... inheritable read for FQAN:/atlas and write for FQAN:/atlas/Role=production /atlas-root/atlaslocalgroupdisk ... inheritable read for FQAN:/atlas and write for FQAN:/atlas/country_code Example for following X.509 identity mapping FQAN:/atlas gid 2000, FQAN:/atlas/Role=production gid 2001, FQAN:/atlas/cz gid 2002 $ chimera ls / dr-xr-x--- 19 2000 2000 512 May 10 00:00 atlas-root $ chimera ls /atlas-root drwxr-xr-x 10 2001 2001 512 Jun 01 2020 atlasdatadisk drwxr-xr-x 5 2001 2001 512 Jan 18 2020 atlaslocalgroupdisk drwxr-xr-x 60 2001 2001 512 Aug 19 2021 atlasscratchdisk $ chimera getfacl /atlas-root/atlasscratchdisk GROUP:2000:+lfsxDd:fdg $ chimera getfacl /atlas-root/atlasdatadisk GROUP:2001:+lfsxDd:fdg GROUP:2000:+lx:fdg $ chimera getfacl /atlas-root/atlaslocalgroupdisk GROUP:2002:+lfsxDd:fdg GROUP:2001:+lfsxDd:fdg GROUP:2000:+lx:fdg","title":"dCache"},{"location":"token-based-authorization/configuration/dcache/#dcache","text":"Official documentation currently doesn't provide all necessary information how to configured storage to accept and map WLCG JWT tokens (see dCache#6607 ). dCache provides two different gPlazma plugins to deal with OIDC tokens - oidc and scitoken . With dCache 7.x it is necessary to use scitoken plugin, because this is the only way to deal with WLCG JWT tokens or SCITOKENS. The dCache 8.x comes with more generic oidc plugin that can process also WLCG JWT tokens and SCITOKENS ( scitoken plugin is still kept for compatibility reasons, but it is deprecated).","title":"dCache"},{"location":"token-based-authorization/configuration/dcache/#dcache-72-configuration","text":"To accept connection authorized with WLCG JWT tokens it is necessary to add scitoken plugin in the chain of gPlazma auth plugins. Just add following line in your /etc/dcache/gplazma.conf configuration auth optional scitoken map sufficient multimap gplazma.multimap.file=/etc/dcache/multi-mapfile.wlcg_jwt This line alone is not sufficient for gPlazma service configuration, because it is necessary to specify supported WLCG JWT issuers. This can be done in the corresponding layout file with defined gPlazma service e.g. # ... [centralDomain/gplazma] # assuming that VO starts in top level directory gplazma.scitoken.issuer!wlcg = https://wlcg.cloud.cnaf.infn.it/ /wlcg gplazma.scitoken.issuer!altas = https://atlas-auth.web.cern.ch/ /atlas gplazma.scitoken.issuer!cms = https://cms-auth.web.cern.ch/ /cms # assuming that dCache WebDAV service runs on default HTTPS port 443 for doors dcache.example.com #gplazma.scitoken.audience-targets = https://dcache.example.com # you can specify multiple audiences, but avoid using generic https://wlcg.cern.ch/jwt/v1/any on production instances # (https://wlcg.cern.ch/jwt/v1/any is necessary for compliance testbed) gplazma.scitoken.audience-targets = https://wlcg.cern.ch/jwt/v1/any https://dcache.example.com https://dcache.example.com:2880 https://alias.example.com # ... It is necessary to map identity extracted from WLCG JWT token (in this case we rely on multimap \" op \" that gives us access to the token issuer mapped to our name configured in the layout file) to the dCache uid , gid and username , e.g. by using multimap file /etc/dcache/multi-mapfile.wlcg_jwt op:wlcg uid:1999 gid:1999,true username:wlcg_oidc op:atlas uid:2999 gid:2999,true username:atlas_oidc op:cms uid:3999 gid:3999,true username:cms_oidc Be wery careful how you map WLCG JWT token indentity and when you support also X.509 voms proxies. Most probably it'll be necessary to very carefully add additional ACLs to your VO (sub)directories.","title":"dCache 7.2 configuration"},{"location":"token-based-authorization/configuration/dcache/#dcache-82-configuration","text":"FIXME: this configuration was not yet tested auth optional oidc map sufficient multimap gplazma.multimap.file=/etc/dcache/multi-mapfile.wlcg_jwt # ... [centralDomain/gplazma] # assuming that VO starts in top level directory gplazma.oidc.provider!wlcg = https://wlcg.cloud.cnaf.infn.it/ -profile=wlcg -prefix=/wlcg gplazma.oidc.provider!altas = https://atlas-auth.web.cern.ch/ -profile=wlcg -prefix=/atlas gplazma.oidc.provider!cms = https://cms-auth.web.cern.ch/ -profile=wlcg -prefix=/cms # assuming that dCache WebDAV service runs on default HTTPS port 443 for doors dcache.example.com #gplazma.oidc.audience-targets = https://dcache.example.com # you can specify multiple audiences, but avoid using generic https://wlcg.cern.ch/jwt/v1/any on production instances # (https://wlcg.cern.ch/jwt/v1/any is necessary for compliance testbed) gplazma.oidc.audience-targets = https://wlcg.cern.ch/jwt/v1/any https://dcache.example.com https://dcache.example.com:2880 https://alias.example.com # ...","title":"dCache 8.2 configuration"},{"location":"token-based-authorization/configuration/dcache/#acl-configuration","text":"To be able to use ACLs with dCache it is necessary to enable them in the dcache.conf # enable ACL support pnfsmanager.enable.acl = true","title":"ACL configuration"},{"location":"token-based-authorization/configuration/dcache/#wlcg-compliance-testbed","text":"Storage configuration for compliance tests.","title":"WLCG compliance testbed"},{"location":"token-based-authorization/configuration/dcache/#atlas","text":"/atlas-root ... read only for /atlas group /atlas-root/atlasscratchdisk ... inheritable read for FQAN:/atlas and write for FQAN:/atlas /atlas-root/atlasdatadisk ... inheritable read for FQAN:/atlas and write for FQAN:/atlas/Role=production /atlas-root/atlaslocalgroupdisk ... inheritable read for FQAN:/atlas and write for FQAN:/atlas/country_code Example for following X.509 identity mapping FQAN:/atlas gid 2000, FQAN:/atlas/Role=production gid 2001, FQAN:/atlas/cz gid 2002 $ chimera ls / dr-xr-x--- 19 2000 2000 512 May 10 00:00 atlas-root $ chimera ls /atlas-root drwxr-xr-x 10 2001 2001 512 Jun 01 2020 atlasdatadisk drwxr-xr-x 5 2001 2001 512 Jan 18 2020 atlaslocalgroupdisk drwxr-xr-x 60 2001 2001 512 Aug 19 2021 atlasscratchdisk $ chimera getfacl /atlas-root/atlasscratchdisk GROUP:2000:+lfsxDd:fdg $ chimera getfacl /atlas-root/atlasdatadisk GROUP:2001:+lfsxDd:fdg GROUP:2000:+lx:fdg $ chimera getfacl /atlas-root/atlaslocalgroupdisk GROUP:2002:+lfsxDd:fdg GROUP:2001:+lfsxDd:fdg GROUP:2000:+lx:fdg","title":"ATLAS"},{"location":"token-based-authorization/configuration/dpm/","text":"DPM Currently supports only very basic configuration described in documentation :exclamation: DPM EOL / end of support comes in summer 2024 (CentOS7/CentOS8 EOL) and it was decided not to fix issues with current WLCG JWT token implementation. If you would like to use WLCG JWT tokens with your production DPM storage than there is a quick, easy and transparent way how to migrate to dCache (several production storages already successfully used this method with less than 48 hours downtime).","title":"DPM"},{"location":"token-based-authorization/configuration/dpm/#dpm","text":"Currently supports only very basic configuration described in documentation :exclamation: DPM EOL / end of support comes in summer 2024 (CentOS7/CentOS8 EOL) and it was decided not to fix issues with current WLCG JWT token implementation. If you would like to use WLCG JWT tokens with your production DPM storage than there is a quick, easy and transparent way how to migrate to dCache (several production storages already successfully used this method with less than 48 hours downtime).","title":"DPM"},{"location":"token-based-authorization/configuration/requirements/","text":"Requirements WLCG Compliance testsuite storage configuration Described in details in the WLCG JWT compliance testsuite README Alice Storage Already use their own proprietary tokens (not WLCG JWT tokens) and that means everything is mapped to one storage identity, because their security model already rely on capabilities. ATLAS Storage Rucio file replication with FTS always use production role when writing files in the \"rucio\" subdirectory and while deleting files. For jobs using rucio upload the identity used while writing files differs for production ( FQAN:/atlas/Role=production identity) and analysis jobs ( FQAN:/atlas identity). Also user can store own files with rucio upload and normal FQAN:/atlas identity. Usual namespace organization with access permission: /atlas-root - read only for FQAN:/atlas group /atlas-root/atlasscratchdisk - inheritable read for FQAN:/atlas and write for FQAN:/atlas + FQAN:/atlas/Role=production /atlas-root/atlasdatadisk - inheritable read for FQAN:/atlas and write for role FQAN:/atlas/Role=production /atlas-root/atlaslocalgroupdisk - inheritable read for FQAN:/atlas and write for group FQAN:/atlas/country_code + FQAN:/atlas/Role=production All files can be read with basic FQAN:/atlas VO identity. Rely exclusively on capabilities that comes with token storage.* scopes and completely ignore wlcg.groups in the token for storage access. Contact: Petr CMS Storage CMS is going to discuss their requirements individually with each storage technology provider. Contact: Stephan LHCb Storage Distinguish two VOMS roles: user and production. /lhcb-root - read/write everywhere with production role FQAN:/lhcb/Role=production , everything readable by FQAN:/lhcb /lhcb-root/user - the only directory writeable by normal users with just basic FQAN:/lhcb Prefer capabilities that comes with token storage.* scopes (depends on Dirac development and may still consider wlcg.groups when it turns out it is too difficult to use capabilities within Dirac framework). Contact: Christophe Belle II Storage Distinguish two VOMS roles: user and production. /belle-root - read/write everywhere with production role FQAN:/belle/Role=production , everything readable by FQAN:/belle except /belle-root/TAPE /belle-root/TAPE - read/write everywhere with production role FQAN:/belle/Role=production , no access by FQAN:/belle /belle-root/TMP - the only directory writeable by normal users with just basic FQAN:/belle Prefer capabilities that comes with token storage.* scopes (depends on Dirac development and may still consider wlcg.groups when it turns out it is too difficult to use capabilities within Dirac framework). Contact: Silvio","title":"Requirements"},{"location":"token-based-authorization/configuration/requirements/#requirements","text":"","title":"Requirements"},{"location":"token-based-authorization/configuration/requirements/#wlcg-compliance-testsuite-storage-configuration","text":"Described in details in the WLCG JWT compliance testsuite README","title":"WLCG Compliance testsuite storage configuration"},{"location":"token-based-authorization/configuration/requirements/#alice-storage","text":"Already use their own proprietary tokens (not WLCG JWT tokens) and that means everything is mapped to one storage identity, because their security model already rely on capabilities.","title":"Alice Storage"},{"location":"token-based-authorization/configuration/requirements/#atlas-storage","text":"Rucio file replication with FTS always use production role when writing files in the \"rucio\" subdirectory and while deleting files. For jobs using rucio upload the identity used while writing files differs for production ( FQAN:/atlas/Role=production identity) and analysis jobs ( FQAN:/atlas identity). Also user can store own files with rucio upload and normal FQAN:/atlas identity. Usual namespace organization with access permission: /atlas-root - read only for FQAN:/atlas group /atlas-root/atlasscratchdisk - inheritable read for FQAN:/atlas and write for FQAN:/atlas + FQAN:/atlas/Role=production /atlas-root/atlasdatadisk - inheritable read for FQAN:/atlas and write for role FQAN:/atlas/Role=production /atlas-root/atlaslocalgroupdisk - inheritable read for FQAN:/atlas and write for group FQAN:/atlas/country_code + FQAN:/atlas/Role=production All files can be read with basic FQAN:/atlas VO identity. Rely exclusively on capabilities that comes with token storage.* scopes and completely ignore wlcg.groups in the token for storage access. Contact: Petr","title":"ATLAS Storage"},{"location":"token-based-authorization/configuration/requirements/#cms-storage","text":"CMS is going to discuss their requirements individually with each storage technology provider. Contact: Stephan","title":"CMS Storage"},{"location":"token-based-authorization/configuration/requirements/#lhcb-storage","text":"Distinguish two VOMS roles: user and production. /lhcb-root - read/write everywhere with production role FQAN:/lhcb/Role=production , everything readable by FQAN:/lhcb /lhcb-root/user - the only directory writeable by normal users with just basic FQAN:/lhcb Prefer capabilities that comes with token storage.* scopes (depends on Dirac development and may still consider wlcg.groups when it turns out it is too difficult to use capabilities within Dirac framework). Contact: Christophe","title":"LHCb Storage"},{"location":"token-based-authorization/configuration/requirements/#belle-ii-storage","text":"Distinguish two VOMS roles: user and production. /belle-root - read/write everywhere with production role FQAN:/belle/Role=production , everything readable by FQAN:/belle except /belle-root/TAPE /belle-root/TAPE - read/write everywhere with production role FQAN:/belle/Role=production , no access by FQAN:/belle /belle-root/TMP - the only directory writeable by normal users with just basic FQAN:/belle Prefer capabilities that comes with token storage.* scopes (depends on Dirac development and may still consider wlcg.groups when it turns out it is too difficult to use capabilities within Dirac framework). Contact: Silvio","title":"Belle II Storage"},{"location":"token-based-authorization/configuration/storm/","text":"StoRM Token-based authentication and authorization is supported by the latest StoRM WebDAV release. Enabling token-based authN/Z for the WLCG IAM instance In order to enable token-based AuthN/Z for the WLCG IAM instance on a StoRM WebDAV deployment, you need to: Add the WLCG issuer to the list of the trusted issuers by the StoRM WebDAV instance. This is done by modifying the /etc/storm/webdav/config/application.yml as follows: oauth: issuers: - name: iam-wlcg issuer: https://wlcg.cloud.cnaf.infn.it/ Enable the issuer for wlcg the storage area, by changing the wlcg storage area configuration /etc/storm/webdav/sa.d/wlcg.properties like in the following example: name=wlcg rootPath=/your/storage/path/ accessPoints=/wlcg vos=wlcg orgs=https://wlcg.cloud.cnaf.infn.it/ authenticatedReadEnabled=false anonymousReadEnabled=false This configuration enables flat authorization access on the storage to members of the WLCG VO, i.e. all users will have read and write access to the data. HTTP TPC support configuration To configure support for HTTP TPC, follow the instructions in the StoRM WebDAV documentation .","title":"StoRM"},{"location":"token-based-authorization/configuration/storm/#storm","text":"Token-based authentication and authorization is supported by the latest StoRM WebDAV release.","title":"StoRM"},{"location":"token-based-authorization/configuration/storm/#enabling-token-based-authnz-for-the-wlcg-iam-instance","text":"In order to enable token-based AuthN/Z for the WLCG IAM instance on a StoRM WebDAV deployment, you need to: Add the WLCG issuer to the list of the trusted issuers by the StoRM WebDAV instance. This is done by modifying the /etc/storm/webdav/config/application.yml as follows: oauth: issuers: - name: iam-wlcg issuer: https://wlcg.cloud.cnaf.infn.it/ Enable the issuer for wlcg the storage area, by changing the wlcg storage area configuration /etc/storm/webdav/sa.d/wlcg.properties like in the following example: name=wlcg rootPath=/your/storage/path/ accessPoints=/wlcg vos=wlcg orgs=https://wlcg.cloud.cnaf.infn.it/ authenticatedReadEnabled=false anonymousReadEnabled=false This configuration enables flat authorization access on the storage to members of the WLCG VO, i.e. all users will have read and write access to the data.","title":"Enabling token-based authN/Z for the WLCG IAM instance"},{"location":"token-based-authorization/configuration/storm/#http-tpc-support-configuration","text":"To configure support for HTTP TPC, follow the instructions in the StoRM WebDAV documentation .","title":"HTTP TPC support configuration"},{"location":"token-based-authorization/configuration/xrootd/","text":"XRootD Token-based authentication and authorization is supported by XRootD in combination with the xrootd-scitokens plugin. Enabling token-based authN/Z for the WLCG IAM instance In order to enable token-based AuthN/Z for the WLCG IAM instance on an XRootD deployment, you need to: Install the xrootd-scitokens plugin. This is for example part of the upstream XRootD yum repository. Add the WLCG issuer to the list of the trusted issuers by the SciTokens library. This can be done by creating a configuration file e.g. at /etc/xrootd/scitokens.cfg with the following content (assuming xrootd should act as the xrootd user and files stored in the /data/grid ): [Global] onmissing = passthrough # don't use https://wlcg.cern.ch/jwt/v1/any on production instances audience = https://xrd.example.com:1094, https://wlcg.cern.ch/jwt/v1/any [Issuer WLCG IAM] issuer = https://wlcg.cloud.cnaf.infn.it/ base_path = /data/grid/wlcg map_subject = false default_user = xrootd Note that the onmissing = passthrough part is needed to continue with other authorization libraries, such as the Macaroons library, and to continue with the evaluation of an authdb file (if used). Extend your existing xrootd configuration file. To stack with the macaroons authentication library, you will need: ofs.authlib ++ libXrdAccSciTokens.so config=/etc/xrootd/scitokens.cfg ofs.authlib ++ libXrdMacaroons.so ofs.authorize 1 # Pass the bearer token to the Xrootd authorization framework. http.header2cgi Authorization authz The ++ is needed for stacking of the authorization libraries. If this is not needed, i.e. you only use one library, the ++ can be dropped. In case an acc.authdb file is used, authorization can be granted as follows, granting full read-write access to the /data/grid/wlcg path and read-only and list access to /data/grid/srr : = wlcgtknusr o: https://wlcg.cloud.cnaf.infn.it/ g: /wlcg x wlcgtknusr /data/grid/wlcg a /data/grid/srr lr This configuration enables flat authorization access on the storage to members of the WLCG VO with the /wlcg group,, i.e. all users will have read and write access to the data at /data/grid/wlcg . WLCG JWT compliance testbed expect also protected resource accessible only with optional group /wlcg/test and this can be configured by following lines in the acc.authdb file for WLCG VO data at /data/grid/wlcg (it is also possible to specify oss.localroot /data/grid and than everything can be configured and used without this prefix) # compound identity to support for group based authorization from WLCG JWT token = wlcgtknprt_token o: https://wlcg.cloud.cnaf.infn.it/ g: /wlcg/test = wlcgtknusr_token o: https://wlcg.cloud.cnaf.infn.it/ g: /wlcg # compound identity to support for mapping X.509 VOMS identity = wlcgtknprt_x509 o: wlcg g: /wlcg r: test = wlcgtknusr_x509 o: wlcg g: /wlcg # templates for accessing normal and protected resources t wlcgtknprt /data/grid/wlcg a /data/grid lr t wlcgtknusr /data/grid/wlcg/protected rl-diknw /data/grid/wlcg a /data/grid lr # configure access for users that comes with X.509 or WLCG JWT token with wlcg.groups # (with \"x\" first matching compound identity is used to grant privileges) x wlcgtknprt_token wlcgtknprt x wlcgtknusr_token wlcgtknusr x wlcgtknprt_x509 wlcgtknprt x wlcgtknusr_x509 wlcgtknusr # WLCG JWT token scope based access is not handled in this configuration file Enabling tokens for ATLAS Simple native XRootD configuration If you use very simple XRootD configuration with posix backend and acc.authdb to specify permission for clients using X.509 VOMS proxy certificate than it should be sufficient to add /etc/xrootd/scitokens.cfg configuration file for xrd.example.com:1094 and top level VO directory /your/base/path/for/atlas # /etc/xrootd/scitokens.cfg [Global] onmissing = passthrough audience = https://xrd.example.com:1094 [Issuer ATLAS] issuer = https://atlas-auth.web.cern.ch/ base_path = /your/base/path/for/atlas map_subject = False default_user = xrootd Plus update ofs.authlib in your XRootD configuration file with libXrdAccSciTokens.so as mentioned in section with WLCG compliance testbed configuration. ATLAS plans to rely exclusively on storage scopes in the tokens and that's why your production acc.authdb configuration should not contain any mapping for wlcg.groups that can be present in the token. All accesses to the storage with tokens that doesn't contain relevant storage scopes should be rejected. EOS mapping with directories using different identity EOS migrated from CASTOR and configured with full compatibility with original CASTOR storage rely only on certificate subject / grid-mapfiles for identity mapping. This configuration could be equally translated for clients that comes with tokens, e.g. /etc/xrootd/scitokens.cfg # /etc/xrootd/scitokens.cfg [Global] onmissing = passthrough audience = https://eosatlas.cern.ch [Issuer ATLAS] issuer = https://atlas-auth.web.cern.ch/ base_path = /eos/atlas map_subject = False name_mapfile = /etc/xrootd/scitokens.map default_user = atlas001 /etc/xrootd/scitokens.map (map all non-default users with different privileges) [ {\"sub\": \"00000000-0000-0000-0000-000000000002\", \"result\": \"atlas002\", \"comment\": \"/DC=ch/DC=cern/OU=Organic Units/OU=Users/CN=user002/CN=000002/CN=Robot: ATLAS User 2\"}, {\"sub\": \"00000000-0000-0000-0000-000000000003\", \"result\": \"atlas003\", \"comment\": \"/DC=ch/DC=cern/OU=Organic Units/OU=Users/CN=user003/CN=000003/CN=Robot: ATLAS User 3\"}, ... ] This kind of mapping assumes that directory owners on EOS match exactly storage scope path restriction defined in IAM. May be it is even better to rely primarily on storage scope path policies defined directly in IAM and than just map paths to the identity that match owner it the EOS namespace, e.g. [ {\"path\": \"/eos/atlas/atlasscratchdisk\", \"result\": \"atlas001\", \"comment\": \"Owner of the ATLASSCRATCHDISK area\"}, {\"path\": \"/eos/atlas/atlasdatadisk\", \"result\": \"atlas003\", \"comment\": \"Owner of the ATLASDATADISK area\"}, ... ]","title":"XRootD"},{"location":"token-based-authorization/configuration/xrootd/#xrootd","text":"Token-based authentication and authorization is supported by XRootD in combination with the xrootd-scitokens plugin.","title":"XRootD"},{"location":"token-based-authorization/configuration/xrootd/#enabling-token-based-authnz-for-the-wlcg-iam-instance","text":"In order to enable token-based AuthN/Z for the WLCG IAM instance on an XRootD deployment, you need to: Install the xrootd-scitokens plugin. This is for example part of the upstream XRootD yum repository. Add the WLCG issuer to the list of the trusted issuers by the SciTokens library. This can be done by creating a configuration file e.g. at /etc/xrootd/scitokens.cfg with the following content (assuming xrootd should act as the xrootd user and files stored in the /data/grid ): [Global] onmissing = passthrough # don't use https://wlcg.cern.ch/jwt/v1/any on production instances audience = https://xrd.example.com:1094, https://wlcg.cern.ch/jwt/v1/any [Issuer WLCG IAM] issuer = https://wlcg.cloud.cnaf.infn.it/ base_path = /data/grid/wlcg map_subject = false default_user = xrootd Note that the onmissing = passthrough part is needed to continue with other authorization libraries, such as the Macaroons library, and to continue with the evaluation of an authdb file (if used). Extend your existing xrootd configuration file. To stack with the macaroons authentication library, you will need: ofs.authlib ++ libXrdAccSciTokens.so config=/etc/xrootd/scitokens.cfg ofs.authlib ++ libXrdMacaroons.so ofs.authorize 1 # Pass the bearer token to the Xrootd authorization framework. http.header2cgi Authorization authz The ++ is needed for stacking of the authorization libraries. If this is not needed, i.e. you only use one library, the ++ can be dropped. In case an acc.authdb file is used, authorization can be granted as follows, granting full read-write access to the /data/grid/wlcg path and read-only and list access to /data/grid/srr : = wlcgtknusr o: https://wlcg.cloud.cnaf.infn.it/ g: /wlcg x wlcgtknusr /data/grid/wlcg a /data/grid/srr lr This configuration enables flat authorization access on the storage to members of the WLCG VO with the /wlcg group,, i.e. all users will have read and write access to the data at /data/grid/wlcg . WLCG JWT compliance testbed expect also protected resource accessible only with optional group /wlcg/test and this can be configured by following lines in the acc.authdb file for WLCG VO data at /data/grid/wlcg (it is also possible to specify oss.localroot /data/grid and than everything can be configured and used without this prefix) # compound identity to support for group based authorization from WLCG JWT token = wlcgtknprt_token o: https://wlcg.cloud.cnaf.infn.it/ g: /wlcg/test = wlcgtknusr_token o: https://wlcg.cloud.cnaf.infn.it/ g: /wlcg # compound identity to support for mapping X.509 VOMS identity = wlcgtknprt_x509 o: wlcg g: /wlcg r: test = wlcgtknusr_x509 o: wlcg g: /wlcg # templates for accessing normal and protected resources t wlcgtknprt /data/grid/wlcg a /data/grid lr t wlcgtknusr /data/grid/wlcg/protected rl-diknw /data/grid/wlcg a /data/grid lr # configure access for users that comes with X.509 or WLCG JWT token with wlcg.groups # (with \"x\" first matching compound identity is used to grant privileges) x wlcgtknprt_token wlcgtknprt x wlcgtknusr_token wlcgtknusr x wlcgtknprt_x509 wlcgtknprt x wlcgtknusr_x509 wlcgtknusr # WLCG JWT token scope based access is not handled in this configuration file","title":"Enabling token-based authN/Z for the WLCG IAM instance"},{"location":"token-based-authorization/configuration/xrootd/#enabling-tokens-for-atlas","text":"","title":"Enabling tokens for ATLAS"},{"location":"token-based-authorization/configuration/xrootd/#simple-native-xrootd-configuration","text":"If you use very simple XRootD configuration with posix backend and acc.authdb to specify permission for clients using X.509 VOMS proxy certificate than it should be sufficient to add /etc/xrootd/scitokens.cfg configuration file for xrd.example.com:1094 and top level VO directory /your/base/path/for/atlas # /etc/xrootd/scitokens.cfg [Global] onmissing = passthrough audience = https://xrd.example.com:1094 [Issuer ATLAS] issuer = https://atlas-auth.web.cern.ch/ base_path = /your/base/path/for/atlas map_subject = False default_user = xrootd Plus update ofs.authlib in your XRootD configuration file with libXrdAccSciTokens.so as mentioned in section with WLCG compliance testbed configuration. ATLAS plans to rely exclusively on storage scopes in the tokens and that's why your production acc.authdb configuration should not contain any mapping for wlcg.groups that can be present in the token. All accesses to the storage with tokens that doesn't contain relevant storage scopes should be rejected.","title":"Simple native XRootD configuration"},{"location":"token-based-authorization/configuration/xrootd/#eos-mapping-with-directories-using-different-identity","text":"EOS migrated from CASTOR and configured with full compatibility with original CASTOR storage rely only on certificate subject / grid-mapfiles for identity mapping. This configuration could be equally translated for clients that comes with tokens, e.g. /etc/xrootd/scitokens.cfg # /etc/xrootd/scitokens.cfg [Global] onmissing = passthrough audience = https://eosatlas.cern.ch [Issuer ATLAS] issuer = https://atlas-auth.web.cern.ch/ base_path = /eos/atlas map_subject = False name_mapfile = /etc/xrootd/scitokens.map default_user = atlas001 /etc/xrootd/scitokens.map (map all non-default users with different privileges) [ {\"sub\": \"00000000-0000-0000-0000-000000000002\", \"result\": \"atlas002\", \"comment\": \"/DC=ch/DC=cern/OU=Organic Units/OU=Users/CN=user002/CN=000002/CN=Robot: ATLAS User 2\"}, {\"sub\": \"00000000-0000-0000-0000-000000000003\", \"result\": \"atlas003\", \"comment\": \"/DC=ch/DC=cern/OU=Organic Units/OU=Users/CN=user003/CN=000003/CN=Robot: ATLAS User 3\"}, ... ] This kind of mapping assumes that directory owners on EOS match exactly storage scope path restriction defined in IAM. May be it is even better to rely primarily on storage scope path policies defined directly in IAM and than just map paths to the identity that match owner it the EOS namespace, e.g. [ {\"path\": \"/eos/atlas/atlasscratchdisk\", \"result\": \"atlas001\", \"comment\": \"Owner of the ATLASSCRATCHDISK area\"}, {\"path\": \"/eos/atlas/atlasdatadisk\", \"result\": \"atlas003\", \"comment\": \"Owner of the ATLASDATADISK area\"}, ... ]","title":"EOS mapping with directories using different identity"}]}