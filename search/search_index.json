{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"The WLCG Virtual Organization","text":"<p>An instance of IAM has been deployed in support of WLCG development and integration activities in support of the migration to token-based authentication and authorization.</p> <p>The WLCG IAM instance is integrated with CERN SSO.</p> <p>Registration is active. </p> <p>IAM documentation is available here.</p> <p>Clients applications can be registered following these instructions, or using oidc-agent.</p> <p>VOMS support is enabled. To link an X.509 certificate to an existing IAM WLCG account, follow these instructions. As in VOMS, multiple certificates can be linked to an account.</p>"},{"location":"#wlcg-vo-voms-configuration","title":"WLCG VO VOMS configuration","text":"<p>New LSC files will be rolled out on Dec. 16th, 2022</p> <p>On Friday Dec. 16th, 2022 at 11 we will update the WLCC VO LSC   configuration. The reason for the update is that the current certificate   is going to expire and that the Sectigo CA INFN currently uses has   changed the structure of the DNs of the issued certificates. </p> <p>The updated configuration that you find here is compliant with the new   certificate. </p> <p>Do not roll out this configuration before Dec. 16th, or VOMS validation   will break at your site.</p> <ul> <li>VOMSES</li> <li>LSC</li> </ul> <p>To have a working VOMS configuration for the WLCG VO:</p> <ol> <li>place the <code>lsc</code> file in the <code>/etc/grid-security/vomsdir/wlcg</code> directory </li> <li>place the <code>vomses</code> file in the <code>/etc/vomses</code> directory (only needed if you    need to do <code>voms-proxy-init</code>)</li> </ol>"},{"location":"#rpm-installation","title":"RPM installation","text":"<p>Outdated starting from Dec. 16th at 11</p> <p>You can use the following RPM package to enable support for the WLCG VO on your RHEL machine:</p> <ul> <li>RPM   package</li> </ul>"},{"location":"#voms-clients-compatibility","title":"VOMS clients compatibility","text":"<p>The latest supported VOMS clients are required.</p> <p>Also note that this VO is supported by IAM, i.e. there are no VOMS Admin endpoints that can be used to generate Gridmap files.</p>"},{"location":"#example","title":"Example","text":"<pre><code>$ voms-proxy-init -voms wlcg\nEnter GRID pass phrase for this identity:\nContacting  wlcg-voms.cloud.cnaf.infn.it:15001 [/DC=org/DC=terena/DC=tcs/C=IT/ST=Roma/O=Istituto Nazionale di Fisica Nucleare - INFN/OU=CNAF/CN=wlcg-voms.cloud.cnaf.infn.it] \"wlcg\"....\nRemote VOMS server contacted succesfully.\n\n\nCreated proxy in /tmp/x509up_u1000.\n\nYour proxy is valid until Tue Nov 29 23:38:44 CET 2022\n\n$ voms-proxy-info -all\nsubject   : /DC=org/DC=terena/DC=tcs/C=IT/O=Istituto Nazionale di Fisica Nucleare - INFN/CN=Federica Agostini fagostin@infn.it/CN=1645960321\nissuer    : /DC=org/DC=terena/DC=tcs/C=IT/O=Istituto Nazionale di Fisica Nucleare - INFN/CN=Federica Agostini fagostin@infn.it\nidentity  : /DC=org/DC=terena/DC=tcs/C=IT/O=Istituto Nazionale di Fisica Nucleare - INFN/CN=Federica Agostini fagostin@infn.it\ntype      : RFC3820 compliant impersonation proxy\nstrength  : 2048\npath      : /tmp/x509up_u1000\ntimeleft  : 11:59:43\nkey usage : Digital Signature, Key Encipherment\n=== VO wlcg extension information ===\nVO        : wlcg\nsubject   : /DC=org/DC=terena/DC=tcs/C=IT/O=Istituto Nazionale di Fisica Nucleare - INFN/CN=Federica Agostini fagostin@infn.it\nissuer    : /DC=org/DC=terena/DC=tcs/C=IT/ST=Roma/O=Istituto Nazionale di Fisica Nucleare - INFN/CN=wlcg-voms.cloud.cnaf.infn.it\nattribute : /wlcg\nattribute : /wlcg/pilots\nattribute : /wlcg/xfers\ntimeleft  : 11:59:43\nuri       : wlcg-voms.cloud.cnaf.infn.it:15001\n</code></pre>"},{"location":"token-based-authorization/","title":"Token-based authorization","text":""},{"location":"token-based-authorization/#getting-an-account-on-the-wlcg-iam-instance","title":"Getting an account on the <code>wlcg</code> IAM instance","text":"<p>To register in the WLCG IAM instance, point your browser to the <code>wlcg</code> IAM VO page.</p> <p>If you have a CERN account, use that account to login to the <code>wlcg</code> VO.</p>"},{"location":"token-based-authorization/#registering-a-client-in-the-wlcg-iam-instance","title":"Registering a client in the WLCG IAM instance","text":"<p>The recommended way of registering a client in the WLCG IAM instance is to use <code>oidc-agent</code> as described here.</p> <p>By default, clients registered in the WLCG IAM instance will use the common WLCG JWT profile.</p>"},{"location":"token-based-authorization/#wlcg-iam-scope-policy-configuration","title":"WLCG IAM scope policy configuration","text":"<p>IAM allows to define policies to limit access to scopes only to selected classes of users.</p> <p>In the IAM <code>wlcg</code> instance policies are defined to limit access to <code>compute.*</code> and <code>storage.*</code> scopes, which are the scopes defined in the WLCG JWT profile to control access to compute and storage resources.</p> <p>In more detail:</p> <ul> <li> <p>access to the <code>compute.create</code>, <code>compute.read</code>, <code>compute.cancel</code>,   <code>compute.modify</code> scope is only granted to members of the <code>wlcg/pilots</code> group;</p> </li> <li> <p>access to the <code>storage.read</code>, <code>storage.modify</code>, <code>storage.create</code> scopes is   only granted to members of the <code>wlcg/xfers</code> group.</p> </li> </ul> <p>Membership in those groups can be requested, after registration, from the IAM dashboard.</p>"},{"location":"token-based-authorization/#getting-tokens-out-iam","title":"Getting tokens out IAM","text":""},{"location":"token-based-authorization/#scope-based-authorization","title":"Scope-based authorization","text":"<p>In order to transfer data across WLCG storage elements using scope-based authorization, request a token with the 'storage.read:/' and 'storage.modify:/' scopes, as follows:</p> <pre><code>$ AT=$(oidc-token -s storage.read:/ -s storage.modify:/ wlcg)\n</code></pre> <p>You can use the <code>jwt</code> utility to inspect the token contents:</p> <pre><code>$ echo $AT | jwt\n...\n\u273b Payload\n{\n  \"wlcg.ver\": \"1.0\",\n  \"sub\": \"a1b98335-9649-4fb0-961d-5a49ce108d49\",\n  \"aud\": \"https://wlcg.cern.ch/jwt/v1/any\",\n  \"nbf\": 1593004542,\n  \"scope\": \"storage.read:/ storage.modify:/\",\n  \"iss\": \"https://wlcg.cloud.cnaf.infn.it/\",\n  \"exp\": 1593008142,\n  \"iat\": 1593004542,\n  \"jti\": \"da0a2f89-3cbf-42a7-9403-0b43d814551d\",\n  \"client_id\": \"edfacfb1-f59d-44d0-9eb6-a745ac52f462\"\n}\n</code></pre> <p>or directly decode token payload with <code>echo $AT | sed 's/.*\\.\\(.*\\)\\..*/\\1==/' | base64 -d | jq</code></p> <p>Then you can use <code>davix</code> or <code>curl</code> to access WLCG storage:</p> <pre><code>$ davix-ls -l --capath /etc/grid-security/certificates -H \"Authorization: Bearer ${AT}\" https://xfer.cr.cnaf.infn.it:8443/wlcg/\ndrwxrwxrwx 0     0          2020-06-24 10:41:13 wlcgdoma\n-rwxrwxrwx 0     1048576    2020-06-18 11:16:25 1M\ndrwxrwxrwx 0     0          2020-04-08 14:56:56 https\ndrwxrwxrwx 0     0          2020-06-24 15:18:36 test-andrea\n\n$ davix-put --capath /etc/grid-security/certificates -H \"Authorization: Bearer ${AT}\" /etc/services https://prometheus.desy.de:2443/VOs/wlcg/file1\n$ davix-get --capath /etc/grid-security/certificates https://prometheus.desy.de:2443/VOs/wlcg/file1 /tmp/x\nPerforming Read operation on: http://[2001:638:700:1005:0:0:1:95]:21881/VOs/wlcg/file1?dcache-http-uuid=679aa9a4-b946-49fd-8522-156d165425c5\n[==================================] 100%     654KiB/654KiB         0B/s\n$ davix-mkdir --capath /etc/grid-security/certificates -H \"Authorization: Bearer ${AT}\" https://golias100.farm.particle.cz/dpm/farm.particle.cz/home/wlcg/dir1\n$ davix-cp --capath /etc/grid-security/certificates -H \"Authorization: Bearer ${AT}\" -H \"TransferHeaderAuthorization: Bearer ${AT}\" --copy-mode pull https://prometheus.desy.de:2443/VOs/wlcg/file1 https://golias100.farm.particle.cz/dpm/farm.particle.cz/home/wlcg/dir1/file1.pull\n$ davix-cp --capath /etc/grid-security/certificates -H \"Authorization: Bearer ${AT}\" -H \"TransferHeaderAuthorization: Bearer ${AT}\" --copy-mode pull https://prometheus.desy.de:2443/VOs/wlcg/file1 https://golias100.farm.particle.cz/dpm/farm.particle.cz/home/wlcg/dir1/file1.push\n$ davix-rm --capath /etc/grid-security/certificates -H \"Authorization: Bearer ${AT}\" https://golias100.farm.particle.cz/dpm/farm.particle.cz/home/wlcg/dir1\n\n# Create directory\n$ curl --capath /etc/grid-security/certificates -X MKCOL -H \"Authorization: Bearer ${AT}\" https://golias100.farm.particle.cz/dpm/farm.particle.cz/home/wlcg/dir1\n# Upload file\n$ curl --capath /etc/grid-security/certificates -L -X PUT -H \"Authorization: Bearer ${AT}\" --upload-file /etc/services https://golias100.farm.particle.cz/dpm/farm.particle.cz/home/wlcg/dir1/file1\n# Download file\n$ curl --capath /etc/grid-security/certificates -L -X GET -H \"Authorization: Bearer ${AT}\" --output /tmp/x https://golias100.farm.particle.cz/dpm/farm.particle.cz/home/wlcg/dir1/file1\n# TPC pull\n$ curl --capath /etc/grid-security/certificates -L -X COPY -H 'Credentials: none' -H \"Authorization: Bearer ${AT}\" -H \"TransferHeaderAuthorization: Bearer ${AT}\" -H \"Source: https://golias100.farm.particle.cz/dpm/farm.particle.cz/home/wlcg/dir1/file1\" https://golias100.farm.particle.cz/dpm/farm.particle.cz/home/wlcg/dir1/file2\n# TPC push\n$ curl --capath /etc/grid-security/certificates -L -X COPY -H 'Credentials: none' -H \"Authorization: Bearer ${AT}\" -H \"TransferHeaderAuthorization: Bearer ${AT}\" -H \"Destination: https://golias100.farm.particle.cz/dpm/farm.particle.cz/home/wlcg/dir1/file3\" https://golias100.farm.particle.cz/dpm/farm.particle.cz/home/wlcg/dir1/file1\n# Delete\n$ curl --capath /etc/grid-security/certificates -L -X DELETE -H \"Authorization: Bearer ${AT}\" https://golias100.farm.particle.cz/dpm/farm.particle.cz/home/wlcg/dir1\n</code></pre> <p>Gfal python API can be also used with tokens, e.g.</p> <pre><code>import os\nimport gfal2\nctx = gfal2.creat_context()\ncred = gfal2.cred_new(\"BEARER\", os.getenv('TOKEN'))\ngfal2.cred_set(ctx, 'amnesiac.cloud.cnaf.infn.it', cred)\nctx.stat('https://amnesiac.cloud.cnaf.infn.it:8443/wlcg/wlcgdoma/https/jwttest/1M')\n</code></pre>"},{"location":"token-based-authorization/#group-based-authorization","title":"Group-based authorization","text":"<p>TBD</p>"},{"location":"token-based-authorization/compliance/","title":"WLCG JWT compliance","text":""},{"location":"token-based-authorization/compliance/#joining-compliance-tests","title":"Joining compliance tests","text":"<p>Create pull request in the wlcg-jwt-compliance-tests repository with your storage configuration or by sending mail to wlcg-doma-tpc e-group with details how to access your storage endpoint.</p>"},{"location":"token-based-authorization/compliance/#storage-pre-requirements","title":"Storage pre-requirements","text":"<p>Storage endpoint used in these tests must follow testsuite storage area configuration pre-requisites.</p>"},{"location":"token-based-authorization/compliance/#results","title":"Results","text":"<p>WLCG JWT compliance test suite runs every day at 12 UTC and results can be accessed using CNAF Jenkins dashboard.</p>"},{"location":"token-based-authorization/doma-testbed/","title":"DOMA Token-based AuthZ Testbed","text":""},{"location":"token-based-authorization/doma-testbed/#joining-testbed","title":"Joining testbed","text":"<p>Send mail to wlcg-doma-tpc e-group if you want to join Rucio DOMA Functional Tests OIDC or advertise your CE that support job submission with tokens. Please include necessary to access your SE/CE endpoint, as an example you can use entries that were already filled in tables with available resources that to some degree already supports tokens.</p>"},{"location":"token-based-authorization/doma-testbed/#iam","title":"IAM","text":"<p>To be able to use tokens it is necessary to know which token issuer is used by individual VOs. For tests we use WLCG IAM token issuer which provides services for \"wlcg\" VO, e.g.</p> VO Issuer WLCG IAM https://wlcg.cloud.cnaf.infn.it/ ATLAS IAM https://atlas-auth.web.cern.ch/ CMS IAM https://cms-auth.web.cern.ch/ DUNE https://cilogon.org/dune Fermilab https://cilogon.org/fermilab <p>OSG have token isser details stored with all VO detail in their virtual-organization topology files. WLCG/EGI currently doesn't provide common (trusted) place with token isser details, but naturally this could become part of EGI VO ID Cards (briefly touched this topic here).</p>"},{"location":"token-based-authorization/doma-testbed/#ce","title":"CE","text":""},{"location":"token-based-authorization/doma-testbed/#arc-ce","title":"ARC-CE","text":"<p>ARC-CE 6.12 still has limited support for WLCG JWT tokens and they can be only used to submit jobs with HTTP based protocols (<code>EMI-ES</code> and <code>REST</code> interface). With current limitations configuration close to expectations from WLCG JWT profile could look like (replace <code>arc1.example.com</code> with your ARC-CE hostname or <code>arc1.example.com:port_number</code> if you don't use standard HTTPS port <code>443</code>):</p> <pre><code># ...\n[authtokens]\n\n[authgroup: wlcg_iam]\n# capability based authorization that use compute.* scopes\nauthtokens = * https://wlcg.cloud.cnaf.infn.it/ https://arc1.example.com compute.create *\nauthtokens = * https://wlcg.cloud.cnaf.infn.it/ https://arc1.example.com compute.read *\nauthtokens = * https://wlcg.cloud.cnaf.infn.it/ https://arc1.example.com compute.modify *\nauthtokens = * https://wlcg.cloud.cnaf.infn.it/ https://arc1.example.com compute.cancel *\n# group based authorization that use /wlcg/pilots group (LHC experiments prefer capabilities)\nauthtokens = * https://wlcg.cloud.cnaf.infn.it/ https://arc1.example.com * /wlcg/pilots\n\n# accept token issued by EGI Check-in for job submission (both old MitreID and new Keycloak issuer)\n[authgroup: egi_aai]\n# very rough / DANGEROUS configuration - accepting all tokens without restrictions\n#authtokens = * https://aai.egi.eu/oidc/ * * *\n#authtokens = * https://aai.egi.eu/auth/realms/egi * * *\n# it is possible to restrict job submission to the specific EGI user\nauthtokens = 85ff127e07ea6660c727831b99e18e4e96b319283f8d2cc8113f405bad2ba261@egi.eu https://aai.egi.eu/oidc/ * * *\nauthtokens = 85ff127e07ea6660c727831b99e18e4e96b319283f8d2cc8113f405bad2ba261@egi.eu https://aai.egi.eu/auth/realms/egi * * *\n\n# just an example for ARC-CE running on arc1.example.com\n# recommendation for ATLAS configuration may change in fugure\n# (this is not the official ATLAS site configuration documentation)\n[authgroup: atlas_iam_prd]\nauthtokens = 7dee38a3-6ab8-4fe2-9e4c-58039c21d817 https://atlas-auth.web.cern.ch/ https://arc1.example.com compute.create *\nauthtokens = 7dee38a3-6ab8-4fe2-9e4c-58039c21d817 https://atlas-auth.web.cern.ch/ https://arc1.example.com compute.read *\nauthtokens = 7dee38a3-6ab8-4fe2-9e4c-58039c21d817 https://atlas-auth.web.cern.ch/ https://arc1.example.com compute.modify *\nauthtokens = 7dee38a3-6ab8-4fe2-9e4c-58039c21d817 https://atlas-auth.web.cern.ch/ https://arc1.example.com compute.cancel *\n[authgroup: atlas_iam_plt]\nauthtokens = 750e9609-485a-4ed4-bf16-d5cc46c71024 https://atlas-auth.web.cern.ch/ https://arc1.example.com compute.create *\nauthtokens = 750e9609-485a-4ed4-bf16-d5cc46c71024 https://atlas-auth.web.cern.ch/ https://arc1.example.com compute.read *\nauthtokens = 750e9609-485a-4ed4-bf16-d5cc46c71024 https://atlas-auth.web.cern.ch/ https://arc1.example.com compute.modify *\nauthtokens = 750e9609-485a-4ed4-bf16-d5cc46c71024 https://atlas-auth.web.cern.ch/ https://arc1.example.com compute.cancel *\n[authgroup: atlas_iam_sgm]\nauthtokens = 5c5d2a4d-9177-3efa-912f-1b4e5c9fb660 https://atlas-auth.web.cern.ch/ https://arc1.example.com compute.create *\nauthtokens = 5c5d2a4d-9177-3efa-912f-1b4e5c9fb660 https://atlas-auth.web.cern.ch/ https://arc1.example.com compute.read *\nauthtokens = 5c5d2a4d-9177-3efa-912f-1b4e5c9fb660 https://atlas-auth.web.cern.ch/ https://arc1.example.com compute.modify *\nauthtokens = 5c5d2a4d-9177-3efa-912f-1b4e5c9fb660 https://atlas-auth.web.cern.ch/ https://arc1.example.com compute.cancel *\n\n# again, just an example for ARC-CE running on arc1.example.com\n# (this is not the official CMS site configuration documentation)\n[authgroup: cms_iam_pilot]\nauthtokens = bad55f4e-602c-4e8d-a5c5-bd8ffb762113 https://cms-auth.web.cern.ch/ https://arc1.example.com compute.create *\nauthtokens = bad55f4e-602c-4e8d-a5c5-bd8ffb762113 https://cms-auth.web.cern.ch/ https://arc1.example.com compute.read *\nauthtokens = bad55f4e-602c-4e8d-a5c5-bd8ffb762113 https://cms-auth.web.cern.ch/ https://arc1.example.com compute.modify *\nauthtokens = bad55f4e-602c-4e8d-a5c5-bd8ffb762113 https://cms-auth.web.cern.ch/ https://arc1.example.com compute.cancel *\n[authgroup: cms_iam_test]\nauthtokens = 08ca855e-d715-410e-a6ff-ad77306e1763 https://cms-auth.web.cern.ch/ https://arc1.example.com compute.create *\nauthtokens = 08ca855e-d715-410e-a6ff-ad77306e1763 https://cms-auth.web.cern.ch/ https://arc1.example.com compute.read *\nauthtokens = 08ca855e-d715-410e-a6ff-ad77306e1763 https://cms-auth.web.cern.ch/ https://arc1.example.com compute.modify *\nauthtokens = 08ca855e-d715-410e-a6ff-ad77306e1763 https://cms-auth.web.cern.ch/ https://arc1.example.com compute.cancel *\n[authgroup: cms_iam_itb]\nauthtokens = 490a9a36-0268-4070-8813-65af031be5a3 https://cms-auth.web.cern.ch/ https://arc1.example.com compute.create *\nauthtokens = 490a9a36-0268-4070-8813-65af031be5a3 https://cms-auth.web.cern.ch/ https://arc1.example.com compute.read *\nauthtokens = 490a9a36-0268-4070-8813-65af031be5a3 https://cms-auth.web.cern.ch/ https://arc1.example.com compute.modify *\nauthtokens = 490a9a36-0268-4070-8813-65af031be5a3 https://cms-auth.web.cern.ch/ https://arc1.example.com compute.cancel *\n\n# this assumes existence of local users wlcg, egi, atlasprd, atlasplt, atlassgm, cmsplt, cmstest and cmsitb with corresponding groups\n[mapping]\nmap_to_user = wlcg_iam wlcg:wlcg\nmap_to_user = egi_aai egi:egi\nmap_to_user = atlas_iam_prd atlasprd:atlasprd\nmap_to_user = atlas_iam_plt atlasplt:atlasplt\nmap_to_user = atlas_iam_sgm atlassgm:atlassgm\nmap_to_user = cms_iam_pilot cmsplt:cmsplt\nmap_to_user = cms_iam_test cmstest:cmstest\nmap_to_user = cms_iam_itb cmsitb:cmsitb\npolicy_on_nomap=stop\n\n[arex/ws/jobs]\nallowaccess=wlcg_iam\nallowaccess=egi_aai\nallowaccess=atlas_iam_prd\nallowaccess=atlas_iam_plt\nallowaccess=atlas_iam_sgm\nallowaccess=cms_iam_pilot\nallowaccess=cms_iam_test\nallowaccess=cms_iam_itb\n# ...\n</code></pre> <p>Token implementation in ARC-CE is still preliminary and it seems possible there will be changes even in the structure of configuration file. You should follow official documentation if you install more recent ARC-CE version.</p> <p>Even though job submission is possible with tokens it is still necessary to have existing X.509 proxy, because current <code>arcsub</code> version still unconditionally verify proxy presence. On the other hand this proxy is delegated to ARC-CE regardless of auth mechanism used to submit jobs.</p> <p>With token in <code>BEARER_TOKEN</code> environement variable (<code>ARC_OTOKEN</code> for older ARC-CE 6.7 clients) token'll be automatically used for authorization with HTTP interface, e.g.</p> <pre><code>cat &gt; test.xrsl &lt;&lt;EOF\n&amp;(executable=\"/usr/bin/env\")\n(stdout=\"test.out\")\n(stderr=\"test.err\")\n(jobname=\"ARC-CE test\")\n(runtimeenvironment = \"ENV/PROXY\")\nEOF\n# following line is necessary for ARC-CE 6.7 client\n# newer version use BEARER_TOKEN or token discovery\n#export ARC_OTOKEN=$(oidc-token --aud=https://arc1.farm.particle.cz --scope=compute.read --scope=compute.modify --scope=compute.cancel --scope=compute.create --scope=wlcg.groups:/wlcg/pilots OIDC_STORE_NAME)\nexport BEARER_OTOKEN=$(oidc-token --aud=https://arc1.farm.particle.cz --scope=compute.read --scope=compute.modify --scope=compute.cancel --scope=compute.create --scope=wlcg.groups:/wlcg/pilots OIDC_STORE_NAME)\narcsub --debug=DEBUG --info-endpoint-type arcrest --submission-endpoint-type arcrest --computing-element arc1.farm.particle.cz test.xrsl\n</code></pre> <p>ARC-CE support token discovery as described in WLCG Bearer Token Discovery</p>"},{"location":"token-based-authorization/doma-testbed/#configured-endpoints","title":"Configured endpoints","text":"Site Host VO Audience Group praguelcg2 arc1.farm.particle.cz, arc2.farm.particle.cz wlcg * /wlcg/pilots praguelcg2 arc1.farm.particle.cz, arc2.farm.particle.cz atlas, dune * *"},{"location":"token-based-authorization/doma-testbed/#htcondor-ce","title":"HTCondor-CE","text":"<p>It is possible to submit jobs with WLCG JWT token if HTCondor accepts <code>SCITOKENS</code>. By default HTCondor-CE is configured with non-standard <code>aud</code> claim, but this can be changed by adding following line in <code>/etc/condor-ce/config.d/99-local</code> file</p> <pre><code>SCITOKENS_SERVER_AUDIENCE = $(SCITOKENS_SERVER_AUDIENCE) condor://$(COLLECTOR_HOST)\n# special \"any\" audience, not recommended for production use\n#SCITOKENS_SERVER_AUDIENCE = $(SCITOKENS_SERVER_AUDIENCE) https://wlcg.cern.ch/jwt/v1/any\n</code></pre> <p>All other configuration and identity mapping is described in the documentation. Currently identity mapping is limited to <code>sub</code>+<code>iss</code> which is not sufficient for IAM shared by multiple groups (e.g. Fermialab), but there are plans to implement callout similar to the GSI lcmaps.</p> <p>Follow these steps to submit job with token to HTCondor-CE</p> <pre><code>cat &gt; test.sub &lt;&lt;EOF\nexecutable = /bin/env\noutput = env.out\nerror = env.err\nlog = env.log\nqueue\nEOF\n\n# get access token, e.g. from oidc-agent (see bellow) and store its\n# content in a file used by WLCG bearer token discovery mechanism\noidc-token --aud=condor://osgce3.farm.particle.cz:9619 -s compute.create -s compute.read -s compute.modify -s compute.cancel wlcg-compute &gt; $XDG_RUNTIME_DIR/bt_u$(id -u)\n\nexport CONDOR_CONFIG=/dev/null\nexport GSI_AUTHZ_CONF=/dev/null\nexport _condor_AUTH_SSL_CLIENT_CADIR=/etc/grid-security/certificates\nexport _condor_SEC_CLIENT_AUTHENTICATION_METHODS=SCITOKENS\n\ncondor_ping -verbose -pool osgce3.farm.particle.cz:9619 -name osgce3.farm.particle.cz WRITE\ncondor_submit -pool osgce3.farm.particle.cz:9619 -remote osgce3.farm.particle.cz test.sub\ncondor_q -pool osgce3.farm.particle.cz:9619 -name osgce3.farm.particle.cz\n</code></pre>"},{"location":"token-based-authorization/doma-testbed/#obtain-access-token-with-compute-scopes","title":"Obtain access token with compute.* scopes","text":"<ul> <li>oidc-agent</li> </ul> <pre><code># start oidc-agent (if it is not already running)\neval $(oidc-gen)\n# register new client in oidc-agent with compute.* scopes (one time\n# action, but not all IAM allows user to apply for compute.* scopes)\noidc-gen --iss=https://wlcg.cloud.cnaf.infn.it/ --scope=\"openid offline_access wlcg.groups compute.create compute.read compute.modify compute.cancel\" wlcg-compute\n# obtain token with all necessary scopes and right audience\noidc-token --aud=condor://osgce3.farm.particle.cz:9619 --scope \"compute.create compute.read compute.modify compute.cancel\" wlcg-compute &gt; $XDG_RUNTIME_DIR/bt_u$(id -u)\n</code></pre> <ul> <li>client credentials grant (client registered in IAM with <code>compute.*</code> scopes, grant type \"client credentials\" and response type \"token\")</li> </ul> <pre><code>curl -s --data \"grant_type=client_credentials&amp;client_id=client_id_from_iam_registration&amp;client_secret=client_secret_from_iam_registration&amp;audience=condor://osgce3.farm.particle.cz:9619\" https://wlcg.cloud.cnaf.infn.it/token | jq -r .access_token &gt; $XDG_RUNTIME_DIR/bt_u$(id -u)\n</code></pre>"},{"location":"token-based-authorization/doma-testbed/#configured-endpoints_1","title":"Configured endpoints","text":"<p>All OSG sites should now support job submission with token. You can test your HTCondor-CE configuration with ATLAS jobs submission.</p> Site Host VO Audience osg hosted-ce-chtc-ubuntu.osg.chtc.io:9619 wlcg hosted-ce-chtc-ubuntu.osg.chtc.io:9619 praguelcg2 osgce1.farm.particle.cz:9619 wlcg, atlas, dune osgce1.farm.particle.cz:9619, condor://osgce1.farm.particle.cz:9619, https://wlcg.cern.ch/jwt/v1/any"},{"location":"token-based-authorization/doma-testbed/#storage","title":"Storage","text":""},{"location":"token-based-authorization/doma-testbed/#implementations","title":"Implementations","text":"<ul> <li>dCache (transfer documentation)</li> <li>DPM</li> <li>Echo (CEPH + XRootD plugin)</li> <li>EOS</li> <li>StoRM (configuration instructions)</li> <li>XRootD (configuration instructions)</li> </ul>"},{"location":"token-based-authorization/doma-testbed/#configured-endpoints_2","title":"Configured endpoints:","text":"Site Implementation Host VO Audience praguelcg2 DPM 1.15.0 https://golias100.farm.partile.cz/dpm/farm.particle.cz/home/wlcg wlcg https://wlcg.cern.ch/jwt/v1/any UNL XRootD https://red-gridftp12.unl.edu:1094/user/dteam wlcg DESY Devel dCache 7.1 https://prometheus.desy.de:2443/VOs/wlcg wlcg CNAF Prod StoRM https://xfer.cr.cnaf.infn.it:8443/wlcg wlcg CNAF Devel StoRM https://amnesiac.cloud.cnaf.infn.it:8443/wlcg wlcg CERN ATLAS EOS https://eosatlas.cern.ch:443/eos/atlas/atlasscratchdisk/3rdpartycopy atlas CERN Devel EOS https://eospps.cern.ch:443/eos/opstest/tpc/https wlcg RAL Echo https://ceph-gw8.gridpp.rl.ac.uk:1094/dteam:test/ wlcg Manchester Test DPM https://vm33.in.tier2.hep.manchester.ac.uk:443/dpm/tier2.hep.manchester.ac.uk/home/wlcg wlcg"},{"location":"token-based-authorization/doma-testbed/#fts","title":"FTS","text":"<p>CERN Devel FTS with 3.10.x provides JWT support for WLCG and XDC.</p> <pre><code>fts-rest-whoami --access-token=&lt;token&gt; -s https://fts3-devel.cern.ch:8446\nfts-rest-transfer-submit --access-token=&lt;token&gt; -s https://fts3-devel.cern.ch:8446 &lt;src_url&gt; &lt;dst_url&gt;\nfts-rest-transfer-status --access-token=&lt;token&gt; -s https://fts3-devel.cern.ch:8446\n</code></pre> <p>Be aware that for sucessfull FTS transfer submission with OIDC you also need recent 3.10.x FTS rest client.</p>"},{"location":"token-based-authorization/doma-testbed/#rucio","title":"RUCIO","text":"<p>Install Rucio client with one method described in documentation, e.g.</p> <pre><code># latest rucio client works only with python3\nvirtualenv-3 rucio\nsource rucio/bin/activate\npip3 install rucio-clients\npip3 install gfal2-python\n</code></pre> <p>and save following configuration file in <code>rucio/etc/rucio.cfg</code> for WLCG DOMA Rucio OIDC tests</p> <pre><code>[client]\nrucio_host = https://rucio-doma.cern.ch:443\nauth_host = https://rucio-doma-auth.cern.ch:443\nauth_type = oidc\naccount = wlcg_doma\noidc_issuer = wlcg\nca_cert = /etc/grid-security/certificates\n#ca_cert = /etc/pki/tls/certs/CERN-bundle.pem\n</code></pre> <p>Setup environment for Rucio client installed in virtualenv</p> <pre><code>source rucio/bin/activate\n</code></pre> <p>or if you use different installation method just set <code>RUCIO_HOME</code> environment variable to the base directory with <code>etc/rucio.cfg</code> file</p> <pre><code>export RUCIO_HOME=/base/path/to/rucio\n</code></pre> <p>WLCG IAM account is necessary to access WLCG DOMA Rucio instance and user sub claim (WLCG IAM uuid identity) must be associated with <code>wlcg_doma</code> Rucio account by DOMA Rucio ADMIN. It is also possible to associate user certificate subject with <code>wlcg_doma</code> Rucio account to provide access with WLCG VO X.509 certificate proxy, but for different authorization type it is necessary to update <code>auth_type = x509_proxy</code> in <code>rucio.cfg</code> or setting environment variable <code>RUCIO_AUTH_TYPE=x509_proxy</code>.</p>"},{"location":"token-based-authorization/doma-testbed/#monitoring","title":"Monitoring","text":"<ul> <li>Functional Tests with WLCG OIDC</li> <li>Rucio DOMA Configuration</li> <li>Rucio DOMA Logs</li> <li>CERN FTS3 Devel</li> <li>FTS Grafana</li> </ul>"},{"location":"token-based-authorization/issues/","title":"Known issues","text":""},{"location":"token-based-authorization/issues/#oidc-agent","title":"oidc-agent","text":"<ul> <li>not available in EPEL repository</li> </ul>"},{"location":"token-based-authorization/issues/#scitokens","title":"Scitokens","text":"<ul> <li>storage.modify scope is not honoured by   scitokens-cpp:   Small, but breaking issue since the scitokens library honours the   <code>storage.write</code> instead of the <code>storage.modify</code> scope.   Breaks WLCG JWT profile compatibility for any software that relies on the   library.</li> </ul>"},{"location":"token-based-authorization/issues/#xrootd","title":"XRootD","text":""},{"location":"token-based-authorization/issues/#storm","title":"StoRM","text":"<ul> <li>StoRM WebDAV should drop Authorization header in TPC   redirects: this is solved and   coming with the next StoRM release scheduled for the beginning of July;</li> <li>StoRM WebDAV leaks file descriptors when Conscrypt is   enabled: This is   fixed in the code; a workaround to avoid this issue is disabling Conscrypt.   This can be done by setting the following variables in   <code>/etc/sysconfig/storm-webdav</code>:</li> </ul> <pre><code>STORM_WEBDAV_USE_CONSCRYPT=\"false\"\nSTORM_WEBDAV_TPC_USE_CONSCRYPT=\"false\"\nSTORM_WEBDAV_ENABLE_HTTP2=\"false\"\n</code></pre>"},{"location":"token-based-authorization/issues/#dcache","title":"dCache","text":""},{"location":"token-based-authorization/issues/#dpm","title":"DPM","text":"<ul> <li>allows <code>HEAD</code> requests with just <code>storage.create:/</code> (mapping internally capabilities to simple ro/rw activity)</li> <li>supports <code>storage.write</code> scope which is not defined in WLCG JWT profile</li> <li>insecure token identity mapping to local uid (only \"sub\" claim considered while only \"sub\"+\"iss\" guaranteed to be unique)</li> </ul>"},{"location":"token-based-authorization/issues/#eos","title":"EOS","text":""},{"location":"token-based-authorization/issues/#rucio","title":"RUCIO","text":"<ul> <li>token with <code>rucio</code> audience used for FTS file transfers (changed to <code>https://wlcg.cern.ch/jwt/v1/any</code> for Rucio DOMA instnace)</li> <li>default <code>scope</code> set to <code>openid profile</code> wich is not sufficient e.g. for <code>rucio upload</code> or <code>rucio download</code></li> <li>fails in case there is expired X.509 proxy in <code>/tmp/x509up_u$(id -u)</code> although <code>auth_type</code> is set to <code>oidc</code></li> <li>use non-standard location for access token <code>/tmp/$(id -u -n)/.rucio_${RUCIO_ACCOUNT}/auth_token_${RUCIO_ACCOUNT}</code></li> <li>CLI OIDC arguments differs from FTS REST client</li> </ul>"},{"location":"token-based-authorization/issues/#fts","title":"FTS","text":""},{"location":"token-based-authorization/issues/#gfal","title":"Gfal","text":"<ul> <li>tokens can't be passed to <code>gfal-utils</code> CLI tools</li> <li>unable to determine which audience and scope <code>storage.*:/path</code> should be in used for SRM+HTTP transfers (TURL returned by SRM srmPrepareToGet + srmPrepareToPut not known before calling <code>copy</code> function)</li> <li>TPC transfers with StoRM active party fails with gfal2 <code>KEEP_ALIVE=True</code> configuration and <code>davs://</code> protocol (can be reproduced also by 2x <code>ctx.stat('davs://storm:8443/non-existing-file')</code> using gfal python API)</li> </ul>"},{"location":"token-based-authorization/oidc-agent/","title":"OIDC Agent","text":"<p>oidc-agent is a set of tools to get and manage OpenID Connect tokens and make them easily usable from the command line. It follows the <code>ssh-agent</code> design, so users can handle OIDC tokens in a similar way as they do with ssh keys.</p>"},{"location":"token-based-authorization/oidc-agent/#installation-instructions","title":"Installation instructions","text":"<p>Releases are available for several RPM and DEB based distribution, plus MacOS and Windows. The KIT Repo Server shows installation instructions for each release</p> <p>This recipe shows how to quickly install <code>oidc-agent</code> on CENTOS 7.</p> <pre><code>yum install --repofrompath oidc,https://repo.data.kit.edu/centos/centos7 oidc-agent\n</code></pre>"},{"location":"token-based-authorization/oidc-agent/#bootstrapping-oidc-agent","title":"Bootstrapping oidc-agent","text":"<p>The first thing to do is to start oidc-agent. This can be done issuing the following command:</p> <pre><code>$ eval $(oidc-agent)\nAgent pid 62088\n</code></pre> <ul> <li>To install OIDC Agent from source or in a Debian/Ubuntu distro, please refer to the KIT Repo Server and to the oidc-agent installation documentation in gitbook</li> </ul>"},{"location":"token-based-authorization/oidc-agent/#how-to-register-a-client","title":"How to register a client","text":"<p>In order to obtain a token out of an OP such as IAM, a user needs a client registered. oidc-agent can automate this step and store client credentials securely on the user machine.</p> <p>A new client is registered using the <code>oidc-gen</code> command, as follows:</p> <pre><code>$ oidc-gen --iss https://wlcg.cloud.cnaf.infn.it --scope max --flow=device  wlcg\n</code></pre> <p>The <code>--flow=device</code> instructs <code>oidc-agent</code> to use the device code flow for the authentication, which is the recommended way with IAM.</p> <p>oidc-agent will use \"dynamic client registration\" to register a new client and store the client credentials and a refresh token locally in encrypted form (the agent will ask for a password from the user).</p> <p>Some IAM instance may not have \"dynamic client registration\" enabled. In this case you may try <code>oidc-gen</code> with the <code>--pub</code> parameter. This makes use of a pre-registered \"public client\", which may be available for that IAM instance.</p>"},{"location":"token-based-authorization/oidc-agent/#how-to-print-a-list-of-all-configured-accounts","title":"How to print a list of all configured accounts","text":"<p>To obtain a list of all configured accounts configured, either <code>oidc-gen --accounts</code> or <code>oidc-add --list</code> can be used. Both of them can use the same flag <code>-l</code></p> <pre><code>$ oidc-gen -l\nThe following account configurations are usable: \nwlcg\n</code></pre>"},{"location":"token-based-authorization/oidc-agent/#how-to-print-a-client-description","title":"How to print a client description","text":"<p>Printing the full client decrypted content can be done by passing the account shortname or the absolute filepath of the account, with <code>oidc-gen --print</code> or simply the <code>-p</code> flag</p> <pre><code>$ oidc-gen -p wlcg\nEnter decryption password for account config 'wlcg': \n{\n        \"name\": \"wlcg\",\n        \"client_name\":  \"oidc-agent:wlcg\",\n        \"issuer_url\":   \"https://wlcg.cloud.cnaf.infn.it/\",\n        \"device_authorization_endpoint\":        \"https://wlcg.cloud.cnaf.infn.it/devicecode\",\n        \"daeSetByUser\": 0,\n        \"client_id\":    \"f062c71e-920d-4b64-8282-a24d4101d8fc\",\n        \"client_secret\":        \"xxxxxxxxxxxxxxxx\",\n        \"refresh_token\":        \"xxxxxxxxxxxxxxxx\",\n        \"cert_path\":    \"\",\n        \"scope\":        \"address openid profile storage.read:/ wlcg eduperson_entitlement storage.create:/ phone offline_access eduperson_scoped_affiliation storage.modify:/ email wlcg.groups\",\n        \"audience\":     \"\",\n        \"redirect_uris\":        [\"edu.kit.data.oidc-agent:/redirect\", \"http://localhost:8080\", \"http://localhost:4242\", \"http://localhost:10088\"],\n        \"username\":     \"\",\n        \"password\":     \"\",\n        \"client_id_issued_at\":  1592322007,\n        \"registration_access_token\":    \"xxxxxxxxxxxxxx\",\n        \"registration_client_uri\":      \"xxxxxxxxxxxxxx\",\n        \"token_endpoint_auth_method\":   \"client_secret_basic\",\n        \"grant_types\":  [\"urn:ietf:params:oauth:grant-type:device_code\", \"refresh_token\"],\n        \"response_types\":       [\"token\"],\n        \"application_type\":     \"web\",\n        \"cert_path\":    \"\",\n        \"audience\":     \"\"\n}\n</code></pre>"},{"location":"token-based-authorization/oidc-agent/#how-to-get-a-token-from-oidc-agent","title":"How to get a token from oidc-agent","text":"<p>Tokens can be obtained using the <code>oidc-token</code> command, as follows:</p> <pre><code>oidc-token wlcg\n</code></pre> <p>This will request a token with all the scopes requested at client registration time. </p>"},{"location":"token-based-authorization/oidc-agent/#limiting-issued-scopes","title":"Limiting issued scopes","text":"<p>To limit the scopes included in the token, the <code>-s</code> flag can be used, as follows:</p> <pre><code>oidc-token -s storage.read:/ wlcg\n</code></pre> <p>In this example the scopes is being limited to <code>storage.read:/</code></p>"},{"location":"token-based-authorization/oidc-agent/#limiting-token-audience","title":"Limiting token audience","text":"<p>The token audience can be limited using the <code>--aud</code> flag,</p> <pre><code>oidc-token --aud example.audience -s storage.read:/ wlcg\n</code></pre> <p>In this example the audience is being defined as <code>example.audience</code></p> <ul> <li>For more usage options please refer to <code>oidc-agent --help</code> or to oidc-agent usage documentation</li> </ul>"},{"location":"token-based-authorization/configuration/dcache/","title":"dCache","text":"<p>OIDC plugin reference describes parameters and configuration options that can be used to deal with clients authorized with tokens. dCache supports several token profiles including WLCG/SciToken profiles. dCache provides two different gPlazma plugins to deal with OIDC tokens that follow WLCG profile - <code>oidc</code> and <code>scitoken</code>. With dCache 7.x it is necessary to use <code>scitoken</code> plugin, because this is the only way to deal with WLCG JWT tokens or SciTokens. The dCache 8.x comes with more generic <code>oidc</code> plugin that can process also WLCG JWT tokens and SciTokens (<code>scitoken</code> plugin is still kept for compatibility reasons, but it is deprecated).</p>"},{"location":"token-based-authorization/configuration/dcache/#wlcg-jwt-compliance-testbed","title":"WLCG JWT compliance testbed","text":"<p>There are several dCache instances already included in the testbed including <code>prometheus.desy.de</code>, which is maitained by dCache developers and it is running <code>master</code> branch.</p>"},{"location":"token-based-authorization/configuration/dcache/#dcache-plugins-for-wlcgscitoken-profile","title":"dCache plugins for WLCG/SciToken profile","text":""},{"location":"token-based-authorization/configuration/dcache/#dcache-72-configuration","title":"dCache 7.2 configuration","text":"<p>WARNING: LHC experiments don't consider dCache 7.x for transfers with tokens.</p> <p>To accept connection authorized with WLCG JWT tokens it is necessary to add <code>scitoken</code> plugin in the chain of gPlazma auth plugins. Just add following line in your <code>/etc/dcache/gplazma.conf</code> configuration</p> <pre><code>auth     optional     scitoken\nmap      sufficient   multimap gplazma.multimap.file=/etc/dcache/multi-mapfile.wlcg_jwt\n</code></pre> <p>This line alone is not sufficient for gPlazma service configuration, because it is necessary to specify supported WLCG JWT issuers. This can be done in the corresponding layout file with defined gPlazma service e.g.</p> <pre><code># ...\n[centralDomain/gplazma]\n# assuming that VO starts in top level directory\ngplazma.scitoken.issuer!wlcg = https://wlcg.cloud.cnaf.infn.it/ /wlcg\ngplazma.scitoken.issuer!altas = https://atlas-auth.web.cern.ch/ /atlas\ngplazma.scitoken.issuer!cms = https://cms-auth.web.cern.ch/ /cms\n# assuming that dCache WebDAV service runs on default HTTPS port 443 for doors dcache.example.com\n#gplazma.scitoken.audience-targets = https://dcache.example.com\n# you can specify multiple audiences (https://wlcg.cern.ch/jwt/v1/any is necessary for compliance testbed)\ngplazma.scitoken.audience-targets = https://wlcg.cern.ch/jwt/v1/any https://dcache.example.com https://dcache.example.com:2880 https://alias.example.com roots://dcache.example.com \n# ...\n</code></pre> <p>It is necessary to map identity extracted from WLCG JWT token (in this case we rely on multimap \"<code>op</code>\" that gives us access to the token issuer mapped to our name configured in the layout file) to the dCache <code>uid</code>, <code>gid</code> and <code>username</code>, e.g. by using multimap file <code>/etc/dcache/multi-mapfile.wlcg_jwt</code></p> <pre><code>op:wlcg               uid:1999 gid:1999,true username:wlcg_oidc\nop:atlas              uid:2999 gid:2999,true username:atlas_oidc\nop:cms                uid:3999 gid:3999,true username:cms_oidc\n</code></pre> <p>Be careful how you map WLCG JWT token indentity when you support also X.509 voms proxies and your dCache rely on ACLs for VO (sub)directories. WLCG JWT tokens allows access based on scope or group authorization and specific requirements wre not considered in the simple mentioned above. The following sections provide some ideas to consider.</p>"},{"location":"token-based-authorization/configuration/dcache/#dcache-8292-configuration","title":"dCache 8.2/9.2 configuration","text":"<p>These dCache versions comes with important updates.</p> <ol> <li>dCache 8.2 comes with WLCG JWT profile support integrated in \"oidc\" gPlazma plugin and <code>root://</code> + <code>roots://</code> protocol configured on the same port</li> <li>Starting with 8.2.7 preferred authorization plugin is set to ZTN and we can simply rely on BEARER_TOKEN variable</li> <li>Symlinks properly handled since 8.2.22</li> <li>WLCG JWT explicit authorization implemented in 8.2.32 and 9.2.0 (needs workaround in IAM token issuer)</li> <li>Recommended for WLCG experiments are 8.2.35+ and 9.2.3+ (versions older than 8.2.22 can't be used with WLCG JWT tokens)</li> </ol> <p>Following minimal configuration adds support to access files with WLCG JWL tokens</p> <pre><code># /etc/dcache/gplazma.conf\n...\nauth     optional     oidc\n...\n</code></pre> <pre><code># /etc/dcache/layouts/layout_file.conf\n# ...\n[centralDomain/gplazma]\n# assuming that VO starts in top level directory\ngplazma.oidc.provider!wlcg = https://wlcg.cloud.cnaf.infn.it/ -profile=wlcg -prefix=/wlcg -authz-id=\"uid:1999 gid:1999 username:wlcg_oidc\"\ngplazma.oidc.provider!altas = https://atlas-auth.web.cern.ch/ -profile=wlcg -prefix=/atlas -authz-id=\"uid:2999 gid:2999 username:atlas_oidc\"\ngplazma.oidc.provider!cms = https://cms-auth.web.cern.ch/ -profile=wlcg -prefix=/cms -authz-id=\"uid:3999 gid:3999 username:cms_oidc\"\n# assuming that dCache WebDAV service runs on default HTTPS port 443 for doors dcache.example.com\n#gplazma.oidc.audience-targets = https://dcache.example.com\n# you can specify multiple audiences (https://wlcg.cern.ch/jwt/v1/any is necessary for compliance testbed)\ngplazma.oidc.audience-targets = https://wlcg.cern.ch/jwt/v1/any https://dcache.example.com https://dcache.example.com:2880 https://alias.example.com roots://dcache.example.com:1094 dcache.example.com ARBITRARY_STRING_REQUESTED_BY_EXPERIMENT\n# ...\n</code></pre> <p>This configuration is sufficient for VOs that rely exclusively on scope based authorization and dCache which is not configured with ownership inheritance create new files with defined <code>uid</code> and <code>gid</code> number.</p> <p>Access to the storage is restricted the the tokens with <code>aud</code> claim that match one of configured <code>gplazma.oidc.audience-targets</code>. It is up to experiments (VOs) to create tokens with desired audience and that's why one configuration for multiple VOs doesn't really weaken security for individual experiments.</p>"},{"location":"token-based-authorization/configuration/dcache/#supporting-x509-and-token-access-at-same-time","title":"Supporting X.509 and token access at same time","text":"<p>There are (at least) two different ways that ensure consistent access for both authorization methods.</p>"},{"location":"token-based-authorization/configuration/dcache/#dcache-ownership-inheritance","title":"dCache ownership inheritance","text":"<p>Configuration option <code>pnfsmanager.enable.inherit-file-ownership</code> can be used to force dCache to assign parent directory owner and group to all new files and subdirectories. This means objects get same owner and groups regardless of client authorization method (access with X.509 VOMS proxy or tokens).</p>"},{"location":"token-based-authorization/configuration/dcache/#acl-configuration","title":"ACL configuration","text":"<p>Alternatively to the dCache ownership inheritance it is also possible to rely on ACLs. ACL support must be first enabled in the <code>dcache.conf</code></p> <pre><code># enable ACL support\npnfsmanager.enable.acl = true\n</code></pre> <p>Default ACLs are also automatically inherited by newly created objects and this functionality is essential for ensuring consistent access for clients authorized with X.509 certificates or tokens.</p>"},{"location":"token-based-authorization/configuration/dcache/#recursive-acl-updates","title":"Recursive ACL updates","text":"<p>Currently it is not possible to modify ACL recursively using <code>chimera</code> interface (dCache#6844), but you can use NFS mounted dCache and <code>nfs4_setfacl</code>, e.g. 1. enable NFS doors service by updating layout configuration file, e.g.</p> <pre><code>[doorsDomain/nfs]\nnfs.version = 4.1\n</code></pre> <ol> <li>export NFS filesystem: <code>echo \"/ 127.0.0.1(rw,no_root_squash)\" &gt;&gt; /etc/exports</code></li> <li>restart dCache: <code>systemctl restart dcache.target</code></li> <li>mount dCache: <code>mount -o acl,rw 127.0.0.1:/ /mnt</code></li> <li>list current ACL configuration:</li> </ol> <pre><code>nfs4_getfacl /mnt/atlas/atlaslocalgroupdisk\nA:fdg:2000:rx\nA:fdg:2001:rwaDdx\nA:fdg:2002:rwaDdx\nA::OWNER@:rwaDxtTcC\nA::GROUP@:rxtc\nA::EVERYONE@:rxtc\n</code></pre> <ol> <li>use listed ACI to recursively update ACL:</li> </ol> <pre><code>nfs4_setfacl -R -P -s A:fdg:2000:rx,A:fdg:2001:rwaDdx,A:fdg:2002:rwaDdx,A:fdg:2099:rwaDdx /mnt/atlas/atlaslocalgroupdisk\n</code></pre> <ol> <li>unmount dCache: <code>umount /mnt</code></li> </ol>"},{"location":"token-based-authorization/configuration/dcache/#supported-authorizations","title":"Supported authorizations","text":"<ol> <li>Issuer based authorization - access can be granted for any token from given issuer regardless of token content by maping <code>op:isser</code></li> <li>Group based authorization - when dCache decodes groups from the token (e.g. from <code>wlcg.groups</code> claim in case of WLCG profile) mapping can be applied to <code>oidcgrp:/vo/group_name</code></li> <li>Scope based authorization - dCache supports access based on storage capabilities defined in several OIDC profiles (e.g. WLCG &amp; SciToken) </li> </ol> <p>WLCG experiments prefers capability security model when it comes to the storage access and that means access with just issuer or group based authorization should be denied. dCache OIDC plugin configured to use WLCG profile (selected by <code>-profile=wlcg</code> parameter) accepts additional configuration parameter <code>authz-id</code> which is used only for tokens that came with explicit AuthZ statements in the scope claim (e.g. <code>storage.read:/foo</code> scope). This parameter can be used to map directly token issuer to the dCache <code>uid</code>, <code>gid</code> and <code>username</code>. Clients that did not came with scope based authorization will not get mapping defined by <code>authz-id</code> and they'll not be authorized to access dCache storage. In case supported VO rely on issuer/group based authorization you could use <code>non-authz-id</code> parameter in the <code>gplazma.oidc.provider</code> configuration or later map <code>op:provider</code> and/or <code>oidcgrp:/vo/group</code> with <code>multimap</code> plugin. Wrong/missing mapping as usually leads to rejected client request.</p> <p>Following changes needs to be applied to the dCache configuration to allow access for tokens with both, scope + group based authorization * gPlazma OIDC provider configuration must be configured with <code>authz-id</code> parameter</p> <pre><code># /etc/dcache/layouts/layout_file.conf\n# ...\n[centralDomain/gplazma]\n# assuming that VO starts in top level directory\ngplazma.oidc.provider!wlcg = https://wlcg.cloud.cnaf.infn.it/ -profile=wlcg -prefix=/wlcg -authz-id=\"username:atlas_oidc_scope_based uid:1000 gid:1000\" -non-authz-id=\"username:atlas_oidc_group_based uid:1100 gid:1100\"\n# ...\n</code></pre> <ul> <li>new multimap mapping for WLCG groups defined in access token</li> </ul> <pre><code># /etc/dcache/gplazma.conf\n...\nauth     optional     oidc\nmap      sufficient   multimap gplazma.multimap.file=/etc/dcache/multi-mapfile.oidc\n...\nsession  sufficient   omnisession\n#session optional     authzdb\n...\n</code></pre> <ul> <li>content of multimap files to map additional groups</li> </ul> <pre><code># /etc/dcache/multi-mapfile.oidc\n# you could define access for tokens that don't even come with any WLCG group using issuer\n#op:wlcg              gid:1200\noidcgrp:/wlcg         gid:1300\noidcgrp:/wlcg/test    gid:1400\n</code></pre> <p>In case dCache still rely on old <code>authzdb</code> plugin than namespace root can be set in <code>/etc/grid-security/storage-authzdb</code>, but you would have to create new group in the <code>gplazma.oidc.provider</code> configuration (e.g. <code>group:atlas_oidc</code>) or in the identity mapfiles to be able to set</p> <pre><code># /etc/grid-security/storage-authzdb\nauthorize atlas_oidc 1000 1000 / / /\n</code></pre> <p>(not tested, author of this document have no sufficient experience with this deprecated module)</p>"},{"location":"token-based-authorization/configuration/dcache/#storage-mapping-and-permissions-configuration","title":"Storage mapping and permissions configuration","text":""},{"location":"token-based-authorization/configuration/dcache/#wlcg-compliance-testbed","title":"WLCG compliance testbed","text":"<p>Storage configuration for compliance tests.</p>"},{"location":"token-based-authorization/configuration/dcache/#atlas","title":"ATLAS","text":"<p>ATLAS rely exclusively on scope based authorization, never include any mapping for <code>op:atlas</code> or <code>oidcgrp:/atlas</code> in your dCache configuration files.</p> <p>With dCache ownership inheritance the configuration is straightforward and it is sufficient to set right ownership only to the top level directories in the root of ATLAS namespace. Most common configuration can be very simple</p> <ul> <li><code>/atlas-root</code> - read only for /atlas group</li> <li><code>/atlas-root/atlasscratchdisk</code> - inheritable read for <code>FQAN:/atlas</code> and write for <code>FQAN:/atlas</code></li> <li><code>/atlas-root/atlasdatadisk</code> - inheritable read for <code>FQAN:/atlas</code> and write for <code>FQAN:/atlas/Role=production</code></li> <li><code>/atlas-root/atlaslocalgroupdisk</code> - inheritable read for <code>FQAN:/atlas</code> and write for <code>FQAN:/atlas/country_code</code></li> </ul> <p>The configuration can be slightly more complex when dCache rely just on ACLs, because then it is necessary to set directory permissions that will be inherited by new directories and files. For example following X.509 identity mapping <code>FQAN:/atlas</code> gid 2000, <code>FQAN:/atlas/Role=production</code> gid 2001, <code>FQAN:/atlas/cz</code> gid 2002 could be used to achieve behavior expected from ATLAS storage</p> <pre><code>$ chimera ls /\ndr-xr-x---   19 2000 2000        512 May 10 00:00 atlas-root\n$ chimera ls /atlas-root\ndrwxr-xr-x   10 2001 2001        512 Jun 01  2020 atlasdatadisk\ndrwxr-xr-x    5 2001 2001        512 Jan 18  2020 atlaslocalgroupdisk\ndrwxr-xr-x   60 2001 2001        512 Aug 19  2021 atlasscratchdisk\n$ chimera getfacl /atlas-root/atlasscratchdisk\nGROUP:2000:+lfsxDd:fdg\n$ chimera getfacl /atlas-root/atlasdatadisk\nGROUP:2001:+lfsxDd:fdg\nGROUP:2000:+lx:fdg\n$ chimera getfacl /atlas-root/atlaslocalgroupdisk\nGROUP:2002:+lfsxDd:fdg\nGROUP:2001:+lfsxDd:fdg\nGROUP:2000:+lx:fdg\n</code></pre> <p>Both configurations works well with clients accessing storage either with X.509 or WLCG tokens and scope based authorization.</p>"},{"location":"token-based-authorization/configuration/dcache/#configuration","title":"configuration","text":"<pre><code># /etc/dcache/layouts/layout_file.conf\n# ...\n# use your own gplazma domain name (this example assumes gPlazma runs in centralDomain)\n[centralDomain/gplazma]\n# VO issuer prefix:\n# assuming that namespace for VO data is stored in the top level directory /atlas\ngplazma.oidc.provider!atlas = https://atlas-auth.web.cern.ch/ -profile=wlcg -prefix=/atlas -authz-id=\"uid:2001 gid:2001 username:atlas_oidc_with_storage_scope\"\n# In case ATLAS VO namespace starts in /pnfs/example.com/atlas than you must use this full prefix\n# in the provider configuration. Using \"/\" prefix (most probably for any VO) is wrong with severe\n# security implications\n#\n# Audience:\n# audience contains union of `aud` claims used by all supported VOs for ATLAS it is currently\n# sufficient to use \"headnode\" (doors) hostname(s) / hostnames defined in the ATLAS Rucio protocol\n# configuration (see `rucio-admin rse info PRAGUELCG2_DATADISK` or ATLAS CRIC web interface\n# https://atlas-cric.cern.ch/core/storageresource/list/), but to be compliant also with WLCG JWT profile\n# requirements (which can become also ATLAS Rucio requirements in the future) for protocols\n# davs://davs.example.com:443/atlas/data and root://xroot.example.com:1094/atlas/data\n# you should use at least following configuration\ngplazma.oidc.audience-targets = https://wlcg.cern.ch/jwt/v1/any https://davs.example.com roots://xroot.example.com:1094 davs.example.com xroot.example.com\n# ...\n</code></pre> <pre><code># /etc/dcache/gplazma.conf\n...\nauth     optional     oidc\n...\n</code></pre>"},{"location":"token-based-authorization/configuration/dpm/","title":"DPM","text":"<p>Currently supports only very basic configuration described in documentation</p> <p>:exclamation: DPM EOL / end of support comes in summer 2024 (CentOS7/CentOS8 EOL) and it was decided not to fix issues with current WLCG JWT token implementation. If you would like to use WLCG JWT tokens with your production DPM storage than there is a quick, easy and transparent way how to migrate to dCache (several production storages already successfully used this method with less than 48 hours downtime).</p>"},{"location":"token-based-authorization/configuration/requirements/","title":"Requirements","text":""},{"location":"token-based-authorization/configuration/requirements/#wlcg-compliance-testsuite-storage-configuration","title":"WLCG Compliance testsuite storage configuration","text":"<p>Described in details in the WLCG JWT compliance testsuite README</p>"},{"location":"token-based-authorization/configuration/requirements/#alice-storage","title":"Alice Storage","text":"<p>Already use their own proprietary tokens (not WLCG JWT tokens) and that means everything is mapped to one storage identity, because their security model already rely on capabilities.</p>"},{"location":"token-based-authorization/configuration/requirements/#atlas-storage","title":"ATLAS Storage","text":"<p>Rucio file replication with FTS always use production role when writing files in the \"rucio\" subdirectory and while deleting files. For jobs using rucio upload the identity used while writing files differs for production (<code>FQAN:/atlas/Role=production</code> identity) and analysis jobs (<code>FQAN:/atlas</code> identity). Also user can store own files with rucio upload and normal <code>FQAN:/atlas</code> identity.</p> <p>Usual namespace organization with access permission:</p> <ul> <li><code>/atlas-root</code> - read only for <code>FQAN:/atlas</code> group</li> <li><code>/atlas-root/atlasscratchdisk</code> - inheritable read for <code>FQAN:/atlas</code> and write for <code>FQAN:/atlas</code> + <code>FQAN:/atlas/Role=production</code></li> <li><code>/atlas-root/atlasdatadisk</code> - inheritable read for <code>FQAN:/atlas</code> and write for role <code>FQAN:/atlas/Role=production</code></li> <li><code>/atlas-root/atlaslocalgroupdisk</code> - inheritable read for <code>FQAN:/atlas</code> and write for group <code>FQAN:/atlas/country_code</code> + <code>FQAN:/atlas/Role=production</code></li> </ul> <p>All files can be read with basic <code>FQAN:/atlas</code> VO identity.</p> <p>Rely exclusively on capabilities that comes with token <code>storage.*</code> scopes and completely ignore <code>wlcg.groups</code> in the token for storage access.</p> <p>Contact: Petr</p>"},{"location":"token-based-authorization/configuration/requirements/#cms-storage","title":"CMS Storage","text":"<p>CMS is going to discuss their requirements individually with each storage technology provider.</p> <p>Contact: Stephan</p>"},{"location":"token-based-authorization/configuration/requirements/#lhcb-storage","title":"LHCb Storage","text":"<p>Distinguish two VOMS roles: user and production.</p> <ul> <li><code>/lhcb-root</code> - read/write everywhere with production role <code>FQAN:/lhcb/Role=production</code>, everything readable by <code>FQAN:/lhcb</code></li> <li><code>/lhcb-root/user</code> - the only directory writeable by normal users with just basic <code>FQAN:/lhcb</code></li> </ul> <p>Prefer capabilities that comes with token <code>storage.*</code> scopes (depends on Dirac development and may still consider <code>wlcg.groups</code> when it turns out it is too difficult to use capabilities within Dirac framework).</p> <p>Contact: Christophe</p>"},{"location":"token-based-authorization/configuration/requirements/#belle-ii-storage","title":"Belle II Storage","text":"<p>Distinguish two VOMS roles: user and production.</p> <ul> <li><code>/belle-root</code> - read/write everywhere with production role <code>FQAN:/belle/Role=production</code>, everything readable by <code>FQAN:/belle</code> except <code>/belle-root/TAPE</code></li> <li><code>/belle-root/TAPE</code> - read/write everywhere with production role <code>FQAN:/belle/Role=production</code>, no access by <code>FQAN:/belle</code></li> <li><code>/belle-root/TMP</code> - the only directory writeable by normal users with just basic <code>FQAN:/belle</code></li> </ul> <p>Prefer capabilities that comes with token <code>storage.*</code> scopes (depends on Dirac development and may still consider <code>wlcg.groups</code> when it turns out it is too difficult to use capabilities within Dirac framework).</p> <p>Contact: Silvio</p>"},{"location":"token-based-authorization/configuration/storm/","title":"StoRM","text":"<p>Token-based authentication and authorization is supported by the StoRM WebDAV version 1.2.1.</p>"},{"location":"token-based-authorization/configuration/storm/#enabling-token-based-authnz-for-the-wlcg-iam-instance","title":"Enabling token-based authN/Z for the WLCG IAM instance","text":"<p>The following StoRM WebDAV configuration is based on the requirements for the WLCG JWT compliance testsuite.</p> <p>In order to enable the token-based AuthN/Z for the WLCG IAM instance on a StoRM WebDAV deployment, you need to:</p> <ul> <li>Add the following properties for the wlcg storage area in the <code>/etc/storm/webdav/sa.d/wlcg.properties</code> configuration file:</li> </ul> <pre><code># Name of the storage area\nname=wlcg\n\n# Root path for the storage area. Files will be served from this path, which must exist and\n# must be accessible from the user that runs the storm webdav service\nrootPath=/storage/wlcg\n\n# Comma-separated list of storage area access points\naccessPoints=/wlcg\n\n# Comma-separated list of VOMS VOs supported in this storage area\nvos=wlcg\n\n# Comma-separated list of OAuth/OpenID Connect token issuers trusted in this storage area. \n# Such organizations must be included also in the list of trusted\n# issuers specified in /etc/storm/webdav/config/application.yml\norgs=https://wlcg.cloud.cnaf.infn.it/\n\n# Enables read access to storage area files to users authenticated using OAuth/OIDC. Defaults to true\norgsGrantReadPermission=false\n\n# Enables scope-based authorization following the rules imposed by the WLCG JWT profile. Defaults to false\nwlcgScopeAuthzEnabled=true\n\n# Enables fine-grained authorization engine. Defaults to false\nfineGrainedAuthzEnabled=true\n</code></pre> <ul> <li>Add the following properties to the common   <code>/etc/storm/webdav/config/application.yml</code> configuration file:</li> </ul> <pre><code># Comma-separated list of OAuth/OpenID Connect token issuers trusted in this storage area. \n# Such issuers must be included also in the list of trusted\n# organizations specified in /etc/storm/webdav/sa.d/wlcg.properties\noauth:\n  issuers:\n  - name: iam-wlcg\n    issuer: https://wlcg.cloud.cnaf.infn.it/\n\n# Check the audience claim as specified in the WLCG Common JWT Profiles\n    enforce-audience-checks: true\n    audiences:\n        - https://wlcg.cern.ch/jwt/v1/any\n        - https://&lt;webdav-hostname&gt;:&lt;webdav-port&gt;\n\n# List of authorization policies which are applied when the 'fineGrainedAuthzEnabled' \n# property is set to true in /etc/storm/webdav/sa.d/wlcg.properties.\n# The first policy that matches an incoming request will be applied\nstorm:\n  authz:\n    policies:\n\n    - sa: wlcg\n      description: Grant all access to /wlcg/protected to /wlcg/test members\n      actions:\n        - all\n      paths:\n        - /protected/**\n      effect: permit\n      principals:\n        - type: fqan\n          params:\n            fqan: /wlcg/Role=test\n        - type: jwt-group\n          params:\n            iss: https://wlcg.cloud.cnaf.infn.it/\n            group: /wlcg/test\n\n    - sa: wlcg\n      description: Allow read access to /wlcg/protected area to wlcg members\n      actions:\n        - read\n        - list\n      paths:\n        - /protected/**\n      effect: permit\n      principals:\n        - type: vo\n          params:\n            vo: wlcg\n        - type: jwt-group\n          params:\n            iss: https://wlcg.cloud.cnaf.infn.it/\n            group: /wlcg\n\n    - sa: wlcg\n      description: Deny write access to /wlcg/protected area\n      actions:\n        - write\n        - delete\n      paths:\n        - /protected/**\n      effect: deny\n      principals:\n        - type: anyone\n\n    - sa: wlcg\n      description: Grant all access to the SA to wlcg members\n      actions:\n        - all\n      effect: permit\n      principals:\n        - type: vo\n          params:\n            vo: wlcg\n        - type: jwt-group\n          params:\n            iss: https://wlcg.cloud.cnaf.infn.it/\n            group: /wlcg\n</code></pre> <p>This configuration enables a fine grained authorization access on the storage to members of the WLCG VO, i.e. users presenting a valid X.509 VOMS proxy and/or a WLCG JWT token issued by the WLCG IAM instance.</p> <p>The list of policies described by the <code>storm.authz.policies.description</code> property are applied to an authorization request when the <code>fineGrainedAuthzEnabled</code> property is set to true. Those rules follows a first-applicable principle, meaning that the first policy found in the list that matches an incoming request will be applied when rendering an authorization decision. Further policies can be added following the StoRM WebDAV  documentation.</p>"},{"location":"token-based-authorization/configuration/storm/#http-tpc-support-configuration","title":"HTTP TPC support configuration","text":"<p>To configure support for HTTP TPC, follow the instructions in the StoRM WebDAV documentation.</p>"},{"location":"token-based-authorization/configuration/storm/#vo-examples","title":"VO Examples","text":""},{"location":"token-based-authorization/configuration/storm/#atlas","title":"ATLAS","text":"<p>TODO: insert example here</p>"},{"location":"token-based-authorization/configuration/xrootd/","title":"XRootD","text":"<p>Token-based authentication and authorization is supported by XRootD in combination with the xrootd-scitokens plugin.</p>"},{"location":"token-based-authorization/configuration/xrootd/#enabling-token-based-authnz-for-the-wlcg-iam-instance","title":"Enabling token-based authN/Z for the WLCG IAM instance","text":"<p>In order to enable token-based AuthN/Z for the WLCG IAM instance on an XRootD deployment, you need to:</p> <ul> <li> <p>Install the <code>xrootd-scitokens</code> plugin. This is for example part   of the upstream XRootD yum repository.</p> </li> <li> <p>Add the WLCG issuer to the list of the trusted issuers by the SciTokens   library. This can be done by creating a configuration file e.g. at   <code>/etc/xrootd/scitokens.cfg</code> with the following content   (assuming <code>xrootd</code> should act as the <code>xrootd</code> user and files stored   in the <code>/data/grid</code>):</p> </li> </ul> <pre><code>[Global]\nonmissing = passthrough\n# don't use https://wlcg.cern.ch/jwt/v1/any on production instances\naudience = https://xrd.example.com:1094, https://wlcg.cern.ch/jwt/v1/any\n\n[Issuer WLCG IAM]\nissuer = https://wlcg.cloud.cnaf.infn.it/\nbase_path = /data/grid/wlcg\nmap_subject = false\ndefault_user = xrootd\n</code></pre> <ul> <li> <p>Note that the <code>onmissing = passthrough</code> part is needed to continue with   other authorization libraries, such as the Macaroons library,   and to continue with the evaluation of an authdb file (if used).</p> </li> <li> <p>Extend your existing xrootd configuration file. To stack with the   macaroons authentication library, you will need:</p> </li> </ul> <pre><code>ofs.authlib ++ libXrdAccSciTokens.so config=/etc/xrootd/scitokens.cfg\nofs.authlib ++ libXrdMacaroons.so \nofs.authorize 1\n\n# Pass the bearer token to the Xrootd authorization framework.\nhttp.header2cgi Authorization authz\n</code></pre> <ul> <li> <p>The <code>++</code> is needed for stacking of the authorization libraries.   If this is not needed, i.e. you only use one library,   the <code>++</code> can be dropped.</p> </li> <li> <p>In case an <code>acc.authdb</code> file is used, authorization can be granted   as follows, granting full read-write access to the <code>/data/grid/wlcg</code>   path and read-only and list access to <code>/data/grid/srr</code>:</p> </li> </ul> <pre><code>= wlcgtknusr o: https://wlcg.cloud.cnaf.infn.it/ g: /wlcg\nx wlcgtknusr /data/grid/wlcg a  /data/grid/srr lr\n</code></pre> <p>This configuration enables flat authorization access on the storage to members of the WLCG VO with the <code>/wlcg</code> group,, i.e. all users will have read and write access to the data at <code>/data/grid/wlcg</code>.</p> <ul> <li>WLCG JWT compliance testbed   expect also <code>protected</code> resource accessible only with optional group   <code>/wlcg/test</code> and this can be configured by following lines in   the <code>acc.authdb</code> file for WLCG VO data at <code>/data/grid/wlcg</code>   (it is also possible to specify <code>oss.localroot /data/grid</code>   and than everything can be configured and used without this prefix)</li> </ul> <pre><code># compound identity to support for group based authorization from WLCG JWT token\n= wlcgtknprt_token o: https://wlcg.cloud.cnaf.infn.it/ g: /wlcg/test\n= wlcgtknusr_token o: https://wlcg.cloud.cnaf.infn.it/ g: /wlcg\n# compound identity to support for mapping X.509 VOMS identity\n= wlcgtknprt_x509 o: wlcg g: /wlcg r: test\n= wlcgtknusr_x509 o: wlcg g: /wlcg\n# templates for accessing normal and protected resources\nt wlcgtknprt /data/grid/wlcg a /data/grid lr\nt wlcgtknusr /data/grid/wlcg/protected rl-diknw /data/grid/wlcg a /data/grid lr\n# configure access for users that comes with X.509 or WLCG JWT token with wlcg.groups\n# (with \"x\" first matching compound identity is used to grant privileges)\nx wlcgtknprt_token wlcgtknprt\nx wlcgtknusr_token wlcgtknusr\nx wlcgtknprt_x509 wlcgtknprt\nx wlcgtknusr_x509 wlcgtknusr\n# WLCG JWT token scope based access is not handled in this configuration file\n</code></pre>"},{"location":"token-based-authorization/configuration/xrootd/#enabling-tokens-for-atlas","title":"Enabling tokens for ATLAS","text":""},{"location":"token-based-authorization/configuration/xrootd/#simple-native-xrootd-configuration","title":"Simple native XRootD configuration","text":"<p>If you use very simple XRootD configuration with posix backend and <code>acc.authdb</code> to specify permission for clients using X.509 VOMS proxy certificate than it should be sufficient to add <code>/etc/xrootd/scitokens.cfg</code> configuration file for <code>xrd.example.com:1094</code> and top level VO directory <code>/your/base/path/for/atlas</code></p> <pre><code># /etc/xrootd/scitokens.cfg\n[Global]\nonmissing = passthrough\naudience = https://xrd.example.com:1094\n\n[Issuer ATLAS]\nissuer = https://atlas-auth.web.cern.ch/\nbase_path = /your/base/path/for/atlas\nmap_subject = False\ndefault_user = xrootd\n</code></pre> <p>Plus update <code>ofs.authlib</code> in your XRootD configuration file with <code>libXrdAccSciTokens.so</code> as mentioned in section with WLCG compliance testbed configuration.</p> <p>ATLAS plans to rely exclusively on storage scopes in the tokens and that's why your production <code>acc.authdb</code> configuration should not contain any mapping for <code>wlcg.groups</code> that can be present in the token. All accesses to the storage with tokens that doesn't contain relevant storage scopes should be rejected.</p>"},{"location":"token-based-authorization/configuration/xrootd/#eos-mapping-with-directories-using-different-identity","title":"EOS mapping with directories using different identity","text":"<p>EOS migrated from CASTOR and configured with full compatibility with original CASTOR storage rely only on certificate subject / grid-mapfiles for identity mapping. This configuration could be equally translated for clients that comes with tokens, e.g.</p> <ul> <li>/etc/xrootd/scitokens.cfg</li> </ul> <pre><code># /etc/xrootd/scitokens.cfg\n[Global]\nonmissing = passthrough\naudience = https://eosatlas.cern.ch\n\n[Issuer ATLAS]\nissuer = https://atlas-auth.web.cern.ch/\nbase_path = /eos/atlas\nmap_subject = False\nname_mapfile = /etc/xrootd/scitokens.map\ndefault_user = atlas001\n</code></pre> <ul> <li>/etc/xrootd/scitokens.map (map all non-default users with different privileges)</li> </ul> <pre><code>[\n  {\"sub\": \"00000000-0000-0000-0000-000000000002\", \"result\": \"atlas002\", \"comment\": \"/DC=ch/DC=cern/OU=Organic Units/OU=Users/CN=user002/CN=000002/CN=Robot: ATLAS User 2\"},\n  {\"sub\": \"00000000-0000-0000-0000-000000000003\", \"result\": \"atlas003\", \"comment\": \"/DC=ch/DC=cern/OU=Organic Units/OU=Users/CN=user003/CN=000003/CN=Robot: ATLAS User 3\"},\n  ...\n]\n</code></pre> <p>This kind of mapping assumes that directory owners on EOS match exactly storage scope path restriction defined in IAM. May be it is even better to rely primarily on storage scope path policies defined directly in IAM and than just map paths to the identity that match owner it the EOS namespace, e.g.</p> <pre><code>[\n  {\"path\": \"/eos/atlas/atlasscratchdisk\", \"result\": \"atlas001\", \"comment\": \"Owner of the ATLASSCRATCHDISK area\"},\n  {\"path\": \"/eos/atlas/atlasdatadisk\", \"result\": \"atlas003\", \"comment\": \"Owner of the ATLASDATADISK area\"},\n  ...\n]\n</code></pre>"}]}